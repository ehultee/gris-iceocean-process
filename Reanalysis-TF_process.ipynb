{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean thermal forcing -- additional reanalysis datasets.\n",
    "Clean ocean TF workflow to process reanalyses, other than the depth-averaged EN4 dataset already provided with Verjans repo.\n",
    "\n",
    "19 Mar 2025 | EHU\n",
    "- Tried ASTE, but tiled format of files made it difficult to read in as a multifile dataset.  Ask Mike for suggestions on processing this, if necessary.\n",
    "- 2 Apr: Tried ORAS5, but only had temperature data by default, and Copernicus data service was not functioning to download salinity.\n",
    "- 4 Apr: Try EN4 with g10 correction.  Vincent provides a depth-averaged TF product; we will try to make one on multiple levels.\n",
    "- 9 Apr: Add 1950-1990 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import csv\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import dask\n",
    "from datetime import datetime\n",
    "\n",
    "from verjansFunctions import freezingPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings for this run\n",
    "saveBoxGreenlandNC = True\n",
    "cwd                = os.getcwd()+'/'\n",
    "\n",
    "SelModel = 'Hadley'\n",
    "\n",
    "data_directory = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/Ocean-reanalyses/Hadley'\n",
    "# DirThetaoNC = data_directory + f'THETA'\n",
    "# DirSoNC     = data_directory + f'SALT/'\n",
    "DirSaveNC   = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/'\n",
    "\n",
    "\n",
    "### Limits of Greenland domain ###\n",
    "limN           = 86.0 ## degrees N latitude\n",
    "limS           = 57.0 ## degrees N latitude\n",
    "limE           = 4.0 ## degrees E latitude\n",
    "limW           = 274.0 ## degrees E latitude\n",
    "## CHECK: confirm that output shows up within this W-E box and not its E-W complement\n",
    "limDp          = 1200.0\n",
    "depthSubSample = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in from multiple files. EN4 comes with one NC file per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all tiles together using multifile dataset\n",
    "## 19 Mar: doesn't work for now.  Try a single file to check structure?\n",
    "with xr.open_mfdataset(f'{data_directory}/g10/EN*.nc') as ds_temp: \n",
    "    ds = ds_temp.load()\n",
    "\n",
    "# with xr.open_dataset(f'{DirThetaoNC}/THETA.'+'{:04d}.nc'.format(tile_numbers[1])) as ds_temp:\n",
    "#     ds = ds_temp.load()\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim the data to Greenland bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the lat/lon included in the bounding box and work only with this subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## trim to Greenland bounding box\n",
    "include_lat = (ds.lat>=limS) & (ds.lat <=limN)\n",
    "include_lon = np.logical_or(((ds.lon%360)<=limE),((ds.lon %360) >=limW)) \n",
    "## modulo 360 to account for lon going -180 to 180 or 0-360\n",
    "\n",
    "with dask.config.set(**{'array.slicing.split_large_chunks': True}): ## mitigate performance problem with slicing\n",
    "    gld_ds = ds.where(include_lat & include_lon, drop=True)\n",
    "\n",
    "# ## load and trim so\n",
    "# with xr.open_dataset(path1, chunks={'lev':10}) as ds1:\n",
    "#     ## trim to Greenland bounding box\n",
    "#     include_lat = (ds1.lat>=limS) & (ds1.lat <=limN)\n",
    "#     include_lon = np.logical_or(((ds1.lon%360)<=limE),((ds1.lon %360) >=limW))\n",
    "    \n",
    "#     with dask.config.set(**{'array.slicing.split_large_chunks': True}): ## mitigate performance problem with slicing\n",
    "#         gld_so = ds1.where(include_lat & include_lon, drop=True) ## trim to Gld\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gld_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the ocean thermal forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp = xr.apply_ufunc(freezingPoint, gld_ds.salinity, gld_ds.depth, dask='parallelized',\n",
    "                   dask_gufunc_kwargs={'allow_rechunk':True})\n",
    "fftf = gld_ds.temperature - 273.15 - fp ## convert from Kelvin to Celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mask and apply a fill value\n",
    "tf_out = fftf.where(gld_ds.temperature<1e10) ## apply Vincent's fill value of 1.1e20\n",
    "## actually, just let xarray do its native processing with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out.assign_attrs(standard_name='TF',\n",
    "                    long_name='Ocean thermal forcing',\n",
    "                    # fillvalue=1.1e20,\n",
    "                    latbounds=[limS, limN],\n",
    "                    lonbounds=[limW,limE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "ds_temp = tf_out.to_dataset(name='TF')\n",
    "# ds_temp.TF.attrs = tf_out.attrs\n",
    "ds_out = ds_temp.assign_attrs(title='Ocean thermal forcing for {}'.format(SelModel),\n",
    "                             summary='TF computed following Verjans code, in a bounding' + \n",
    "                              ' box around Greenland, for ISMIP7 Greenland forcing.' +\n",
    "                              ' This version for {}'.format(SelModel),\n",
    "                             institution='NASA Goddard Space Flight Center',\n",
    "                             creation_date=now.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write NetCDF out\n",
    "Write to a custom filename in the directory specified above.  Remember to rename the file as needed, e.g. for the correct date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_path = '/home/theghub/ehultee/data/'\n",
    "# year_tag = path0.strip('.nc').split('_')[-1] ## take the year tag from the GCM input (only one of the two input DS, but we have tried to make them match!)\n",
    "out_fn = DirSaveNC + 'tf-{}-1950_2020.nc'.format(SelModel)\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    ds_out.to_netcdf(path=out_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy  # Map projections libary\n",
    "import cartopy.crs as ccrs  # Projections list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = out_fn\n",
    "\n",
    "ds_new = xr.open_dataset(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tavg = ds_new.TF.mean(dim='time') \n",
    "tf_tavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tavg.sel(depth=5.02, method='nearest').mean(skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "tf_tavg.sel(depth=5.02, method='nearest').plot(ax=ax, transform=ccrs.PlateCarree(), x='lon', y='lat') ## specify x and y coordinates\n",
    "ax.coastlines(); ax.gridlines();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tavg.sel(depth=5.02, method='nearest').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
