{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236564fb-fc82-49c5-93f2-8630422b55d6",
   "metadata": {},
   "source": [
    "## QDM correction to CESM2-WACCM historical TF\n",
    "This code applies a Quantile Delta Mapping (QDM) bias correction to the historical period of ocean thermal forcing generated with CESM2-WACCM output.  The basis of the QDM correction is the EN4 reanalysis.  We take the 30-year period from 1985-2014 as the climatological baseline for this correction.\n",
    "\n",
    "CESM2-WACCM is in a rotated polar grid with dimensions `nlat, nlon` rather than `lat, lon`.  Use rioxarray to regrid this to match EN4 grid before applying QDM bias correction.  Note: depth/`lev` variable is in cm rather than m by default.  This is corrected in the current code; if you've changed the variable in an earlier processing step, you'll want to adjust accordingly here.\n",
    "\n",
    "\n",
    "16 Jul 2025 | EHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aca593-f9c2-4754-ab2b-04487467d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import cartopy.crs as ccrs ## map projections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rioxarray\n",
    "from rioxarray.rioxarray import affine_to_coords\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a7c19-075e-432b-9461-36c98031d1c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## from cmethods.utils\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Optional, Union, TypeVar\n",
    "\n",
    "XRData_t = (xr.Dataset, xr.DataArray)\n",
    "NPData_t = (list, np.ndarray, np.generic)\n",
    "XRData = TypeVar(\"XRData\", xr.Dataset, xr.DataArray)\n",
    "NPData = TypeVar(\"NPData\", list, np.ndarray, np.generic)\n",
    "MAX_SCALING_FACTOR = 2 ## to allow multiplicative correction?\n",
    "\n",
    "\n",
    "def check_xr_types(obs: XRData, simh: XRData, simp: XRData) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type xarray.core.dataarray.Dataset or xarray.core.dataarray.DataArray\"\n",
    "\n",
    "    if not isinstance(obs, XRData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, XRData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, XRData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def check_np_types(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type list, np.ndarray or np.generic\"\n",
    "\n",
    "    if not isinstance(obs, NPData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, NPData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, NPData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def nan_or_equal(value1: float, value2: float) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the values are equal or at least one is NaN\n",
    "\n",
    "    :param value1: First value to check\n",
    "    :type value1: float\n",
    "    :param value2: Second value to check\n",
    "    :type value2: float\n",
    "    :return: If any value is NaN or values are equal\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    return np.isnan(value1) or np.isnan(value2) or value1 == value2\n",
    "        \n",
    "def ensure_dividable(\n",
    "    numerator: Union[float, np.ndarray],\n",
    "    denominator: Union[float, np.ndarray],\n",
    "    max_scaling_factor: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensures that the arrays can be divided. The numerator will be multiplied by\n",
    "    the maximum scaling factor of the CMethods class if division by zero.\n",
    "\n",
    "    :param numerator: Numerator to use\n",
    "    :type numerator: np.ndarray\n",
    "    :param denominator: Denominator that can be zero\n",
    "    :type denominator: np.ndarray\n",
    "    :return: Zero-ensured division\n",
    "    :rtype: np.ndarray | float\n",
    "    \"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        result = numerator / denominator\n",
    "\n",
    "    if isinstance(numerator, np.ndarray):\n",
    "        mask_inf = np.isinf(result)\n",
    "        result[mask_inf] = numerator[mask_inf] * max_scaling_factor  # type: ignore[index]\n",
    "\n",
    "        mask_nan = np.isnan(result)\n",
    "        result[mask_nan] = 0  # type: ignore[index]\n",
    "    elif np.isinf(result):\n",
    "        result = numerator * max_scaling_factor\n",
    "    elif np.isnan(result):\n",
    "        result = 0.0\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_pdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Compuites and returns the the probability density function :math:`P(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: The vector to get :math:`P(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`P(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The probability densitiy function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the probability density function :math:`P(x)`\n",
    "\n",
    "        >>> from cmethods get_pdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_pdf(x=x, xbins=xbins))\n",
    "        [2, 5, 5]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def get_cdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Computes and returns returns the cumulative distribution function :math:`F(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: Vector to get :math:`F(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`F(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The cumulative distribution function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the cumulative distribution function :math:`F(x)`\n",
    "\n",
    "        >>> from cmethods.utils import get_cdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_cdf(x=x, xbins=xbins))\n",
    "        [0.0, 0.16666667, 0.58333333, 1.]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    cdf = np.insert(np.cumsum(pdf), 0, 0.0)\n",
    "    return cdf / cdf[-1]\n",
    "\n",
    "\n",
    "def get_inverse_of_cdf(\n",
    "    base_cdf: Union[list, np.ndarray],\n",
    "    insert_cdf: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Returns the inverse cumulative distribution function as:\n",
    "    :math:`F^{-1}_{x}\\left[y\\right]` where :math:`x` represents ``base_cdf`` and\n",
    "    ``insert_cdf`` is represented by :math:`y`.\n",
    "\n",
    "    :param base_cdf: The basis\n",
    "    :type base_cdf: list | np.ndarray\n",
    "    :param insert_cdf: The CDF that gets inserted\n",
    "    :type insert_cdf: list | np.ndarray\n",
    "    :param xbins: Probability boundaries\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The inverse CDF\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    return np.interp(insert_cdf, base_cdf, xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c71f0-69e5-45ad-8663-62c9235338d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def quantile_delta_mapping(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    "    n_quantiles: int,\n",
    "    kind: str = \"+\",\n",
    "    **kwargs,\n",
    "    ) -> NPData:\n",
    "    r\"\"\"\n",
    "    Based on https://python-cmethods.readthedocs.io/en/latest/methods.html#quantile-delta-mapping\n",
    "\n",
    "    kind: str, default + for additive, can be set to * for multiplicative\n",
    "    \"\"\"\n",
    "    # check_adjust_called(\n",
    "    #     function_name=\"quantile_delta_mapping\",\n",
    "    #     adjust_called=kwargs.get(\"adjust_called\"),\n",
    "    # )\n",
    "    check_np_types(obs=obs, simh=simh, simp=simp)\n",
    "\n",
    "    if not isinstance(n_quantiles, int):\n",
    "        raise TypeError(\"'n_quantiles' must be type int\")\n",
    "\n",
    "    if kind=='+':\n",
    "        obs, simh, simp = (\n",
    "            np.array(obs),\n",
    "            np.array(simh),\n",
    "            np.array(simp),\n",
    "        )  # to achieve higher accuracy\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", min(np.nanmin(obs), np.nanmin(simh)))\n",
    "\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = abs(global_max - global_min) / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        # calculate exact CDF values of $F_{sim,p}[T_{sim,p}(t)]$\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "        delta = simp - get_inverse_of_cdf(cdf_simh, epsilon, xbins)  # Eq. 1.3\n",
    "        return QDM1 + delta  # Eq. 1.4\n",
    "\n",
    "    if kind=='*':\n",
    "        obs, simh, simp = np.array(obs), np.array(simh), np.array(simp)\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", 0.0)\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = global_max / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "\n",
    "        delta = ensure_dividable(  # Eq. 2.3\n",
    "            simp,\n",
    "            get_inverse_of_cdf(cdf_simh, epsilon, xbins),\n",
    "            max_scaling_factor=kwargs.get(\n",
    "                \"max_scaling_scaling\",\n",
    "                MAX_SCALING_FACTOR,\n",
    "            ),\n",
    "        )\n",
    "        return QDM1 * delta  # Eq. 2.4\n",
    "    raise NotImplementedError(\n",
    "        f\"{kind=} not available for quantile_delta_mapping. Use '+' or '*' instead.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_cmfunc(\n",
    "    method: str,\n",
    "    obs: XRData,\n",
    "    simh: XRData,\n",
    "    simp: XRData,\n",
    "    **kwargs: dict,\n",
    ") -> XRData:\n",
    "    \"\"\"\n",
    "    Internal function used to apply the bias correction technique to the\n",
    "    passed input data.\n",
    "    \"\"\"\n",
    "    ## hard-code the QDM method\n",
    "    if method!='quantile_delta_mapping':\n",
    "        raise UnknownMethodError('Not implemented for methods other than quantile_delta_mapping')\n",
    "        ## give this a default for what we want to do\n",
    "    else:\n",
    "        method='quantile_delta_mapping' ## not actually going to use this\n",
    "    \n",
    "    check_xr_types(obs=obs, simh=simh, simp=simp)\n",
    "    # if method not in __METHODS_FUNC__:\n",
    "    #     raise UnknownMethodError(method, __METHODS_FUNC__.keys())\n",
    "\n",
    "    if kwargs.get(\"input_core_dims\"):\n",
    "        if not isinstance(kwargs[\"input_core_dims\"], dict):\n",
    "            raise TypeError(\"input_core_dims must be an object of type 'dict'\")\n",
    "        if not len(kwargs[\"input_core_dims\"]) == 3 or any(\n",
    "            not isinstance(value, str) for value in kwargs[\"input_core_dims\"].values()\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                'input_core_dims must have three key-value pairs like: {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}',\n",
    "            )\n",
    "\n",
    "        input_core_dims = kwargs.pop(\"input_core_dims\")\n",
    "    else:\n",
    "        input_core_dims = {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}\n",
    "\n",
    "    result: XRData = xr.apply_ufunc(\n",
    "        quantile_delta_mapping,\n",
    "        obs,\n",
    "        simh,\n",
    "        # Need to spoof a fake time axis since 'time' coord on full dataset is\n",
    "        # different than 'time' coord on training dataset.\n",
    "        simp.rename({input_core_dims[\"simp\"]: \"__t_simp__\"}),\n",
    "        dask=\"parallelized\",\n",
    "        vectorize=True,\n",
    "        # This will vectorize over the time dimension, so will submit each grid\n",
    "        # cell independently\n",
    "        input_core_dims=[\n",
    "            [input_core_dims[\"obs\"]],\n",
    "            [input_core_dims[\"simh\"]],\n",
    "            [\"__t_simp__\"],\n",
    "        ],\n",
    "        # Need to denote that the final output dataset will be labeled with the\n",
    "        # spoofed time coordinate\n",
    "        output_core_dims=[[\"__t_simp__\"]],\n",
    "        kwargs=dict(kwargs),\n",
    "    )\n",
    "\n",
    "    # Rename to proper coordinate name.\n",
    "    result = result.rename({\"__t_simp__\": input_core_dims[\"simp\"]})\n",
    "\n",
    "    # ufunc will put the core dimension to the end (time), so want to preserve\n",
    "    # original order where time is commonly first.\n",
    "    return result.transpose(*obs.rename({input_core_dims[\"obs\"]: input_core_dims[\"simp\"]}).dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fae5d-a8d9-4c62-a6f8-39926ff4bda3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Time utils from Bryan Riel\n",
    "## pasting stuff from iceutils below.\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "def tdec2datestr(tdec_in, returndate=False):\n",
    "    \"\"\"\n",
    "    Convert a decimaly year to an iso date string.\n",
    "    \"\"\"\n",
    "    if isinstance(tdec_in, (list, np.ndarray)):\n",
    "        tdec_list = copy.deepcopy(tdec_in)\n",
    "    else:\n",
    "        tdec_list = [tdec_in]\n",
    "    current_list = []\n",
    "    for tdec in tdec_list:\n",
    "        year = int(tdec)\n",
    "        yearStart = datetime.datetime(year, 1, 1)\n",
    "        if year % 4 == 0:\n",
    "            ndays_in_year = 366.0\n",
    "        else:\n",
    "            ndays_in_year = 365.0\n",
    "        days = (tdec - year) * ndays_in_year\n",
    "        seconds = (days - int(days)) * 86400\n",
    "        tdelta = datetime.timedelta(days=int(days), seconds=int(seconds))\n",
    "        current = yearStart + tdelta\n",
    "        if not returndate:\n",
    "            current = current.isoformat(' ').split()[0]\n",
    "        current_list.append(current)\n",
    "\n",
    "    if len(current_list) == 1:\n",
    "        return current_list[0]\n",
    "    else:\n",
    "        return np.array(current_list)\n",
    "\n",
    "\n",
    "def datestr2tdec(yy=0, mm=0, dd=0, hour=0, minute=0, sec=0, microsec=0, dateobj=None):\n",
    "    \"\"\"\n",
    "    Convert year, month, day, hours, minutes, seconds to decimal year.\n",
    "    \"\"\"\n",
    "    if dateobj is not None:\n",
    "        if type(dateobj) == str:\n",
    "            yy, mm, dd = [int(val) for val in dateobj.split('-')]\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        elif type(dateobj) == datetime.datetime:\n",
    "            attrs = ['year', 'month', 'day', 'hour', 'minute', 'second']\n",
    "            yy, mm, dd, hour, minute, sec = [getattr(dateobj, attr) for attr in attrs]\n",
    "        elif type(dateobj) == np.datetime64:\n",
    "            yy = dateobj.astype('datetime64[Y]').astype(int) + 1970\n",
    "            mm = dateobj.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "            days = (\n",
    "                (dateobj - dateobj.astype('datetime64[M]')) / np.timedelta64(1, 'D')\n",
    "            )\n",
    "            dd = int(days) + 1\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        else:\n",
    "            raise NotImplementedError('dateobj must be str, datetime, or np.datetime64.')\n",
    "\n",
    "    # Make datetime object for start of year\n",
    "    yearStart = datetime.datetime(yy, 1, 1, 0, 0, 0)\n",
    "    # Make datetime object for input time\n",
    "    current = datetime.datetime(yy, mm, dd, hour, minute, sec, microsec)\n",
    "    # Compute number of days elapsed since start of year\n",
    "    tdelta = current - yearStart\n",
    "    # Convert to decimal year and account for leap year\n",
    "    if yy % 4 == 0:\n",
    "        return float(yy) + tdelta.total_seconds() / (366.0 * 86400)\n",
    "    else:\n",
    "        return float(yy) + tdelta.total_seconds() / (365.0 * 86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3e1d9-d32c-43d4-98fe-453b57cf72a4",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2b62c-dc36-4101-b755-09d257c01176",
   "metadata": {},
   "outputs": [],
   "source": [
    "SelModel = 'CESM'\n",
    "BaselinePeriod = ['1985','2014'] ## climatological baseline for QDM\n",
    "YearsIncluded = ['1850', '2014'] ## set these to the endpoints if you want to slice the CESM2 data.\n",
    "## EN4 data run 1950-2014, but we will use it only during the BaselinePeriod anyway\n",
    "\n",
    "DirSave = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/Summer25Test'\n",
    "DirIn = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/Summer25Test'\n",
    "\n",
    "DirHadley = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles'\n",
    "HadleyFile = f'/tf-Hadley-1950_2020.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1016d0-9be8-4745-bfcf-06b0c09b86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load EN4 using xarray\n",
    "ds1 = xr.open_dataset(DirHadley+HadleyFile, decode_times='timeDim')\n",
    "ds1\n",
    "# ds2 = ds.assign_coords({'timeDim': ds.time, \n",
    "#                   'latDim': ds.lat, \n",
    "#                   'lonDim': ds.lon,\n",
    "#                   'depthDim': ds.depth})\n",
    "\n",
    "# tfEN4 = ds2.tfdpavg0to500_bathymin100.rename({'timeDim': 'time',\n",
    "#                                               'latDim': 'lat',\n",
    "#                                               'lonDim': 'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76909c05-e896-4ead-a285-03f0f42e7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in CESM TF for all time slices available, using multifile dataset\n",
    "with xr.open_mfdataset(f'{DirIn}/tf-CESM2*.nc') as ds: \n",
    "    ds3 = ds.load()\n",
    "\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023e615-b000-4f38-bac1-5df8f06f1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_m = ds3.where(ds3.TF<1e20)\n",
    "ds_m.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fb8d8-20eb-462e-a0ad-852447dbe9da",
   "metadata": {},
   "source": [
    "Depth resampling seemed to be smearing fill value across depth levels, so that the mean of depth-resampled dataset was ~5e17, even when fill values masked out.  Created `ds_m`  before resampling to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a2339-4bb6-479b-9583-7434c0b11de6",
   "metadata": {},
   "source": [
    "## Express depth in m, then resample to Hadley depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298aff7-1fa0-4bf9-927d-7974b9c709a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## resample CESM to Hadley depths?\n",
    "tf_CESM_inM = ds_m.assign_coords(new_depth = ('lev', ds3.indexes['lev'].values*0.01))\n",
    "tf_CESM_inM = tf_CESM_inM.drop_indexes('lev')\n",
    "tf_CESM_inM = tf_CESM_inM.set_xindex('new_depth').drop_vars('lev')\n",
    "tf_CESM_inM = tf_CESM_inM.rename({'new_depth': 'lev'})\n",
    "tf_CESM_inM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64ea12-1605-4f9c-bb86-3ff4364b7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfCESM_resampled = tf_CESM_inM.interp(lev=ds1.depth.values[0:30]).rename({'lev': 'depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8817c1-9f97-436e-a044-28ba5b4af554",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfCESM_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25293c48-28b4-4def-a4ca-85348b366bc6",
   "metadata": {},
   "source": [
    "We had to do one extra step (above) to deal with re-scaling the native depth dimension from cm to m. Then we re-sampled to depth levels that match EN4. Now we proceed with applying a DateTimeIndex, and reprojecting from the rotated pole spatial grid to a regular grid matching EN4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4553ab-0a8e-469a-bca6-7fcec4084128",
   "metadata": {},
   "source": [
    "### Apply DateTimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180b978-d7ed-4119-b9f5-2717422b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_full = tfCESM_resampled.TF.sel(time=slice(YearsIncluded[0], YearsIncluded[1]))\n",
    "\n",
    "## aligning the time indices\n",
    "test_ds_full = test_ds_full.assign_coords(new_time = ('time', test_ds_full.indexes['time'].to_datetimeindex().values))\n",
    "test_ds_full = test_ds_full.drop_indexes('time')\n",
    "test_ds_full = test_ds_full.set_xindex('new_time').drop_vars('time')\n",
    "\n",
    "## aligning the names of the variables between obs and sim\n",
    "test_ds_full = test_ds_full.to_dataset()\n",
    "test_ds_full = test_ds_full.rename({'new_time': 'time'})\n",
    "test_ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54f475-c63a-491e-9a70-a886609afd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tobs_ds_full = ds1.TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670518b-f9eb-422e-a48d-9ea25ec49d65",
   "metadata": {},
   "source": [
    "## Reproject obs to match CESM grid -- on depth slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee14242-e3b1-40ac-80b1-8a2ba4819470",
   "metadata": {},
   "source": [
    "Because the grids are offset from each other, we will need to warp/resample using `rioxarray` before we run the QDM correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d1244-4a1e-4ac7-8991-5aee6d93b170",
   "metadata": {},
   "source": [
    "Note that the rotated-pole grid of CESM2-WACCM is different from the original test case.  We need to get it on a rectilinear grid before we can use `reproject_match`. Follow [this Gist](https://gist.github.com/j08lue/e792b3c912c33e9191734af7e795b75c) using `rasterio.warp` with `src_geoloc_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19352004-bb84-4cae-b22f-a45a83335142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f9f4e-f6f5-4e8a-9695-de3651e7db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm that this version has src_geoloc_array option -- must be 1.4 or greater\n",
    "rasterio.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81258e-bd18-436f-ad57-7a81627f8812",
   "metadata": {},
   "source": [
    "#### Set up reusable quantities outside the loop\n",
    "Shape of the original transform, the coordinates to be applied to our new dataset, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac14a33-c9d6-4424-ab04-c06705e92ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon2d = test_ds_full['lon'].values ## 2d, curvilinear\n",
    "lat2d = test_ds_full['lat'].values\n",
    "\n",
    "lon_EN4 = tobs_ds_full['lon'].values ## 1d, rectilinear grid\n",
    "lat_EN4 = tobs_ds_full['lat'].values\n",
    "\n",
    "WGS84 = rasterio.crs.CRS.from_epsg(4326)\n",
    "\n",
    "## immediately write the spatial reference to tobs_ds_full for later use\n",
    "tobs_ds_full.rio.write_crs(WGS84, inplace=True).rio.set_spatial_dims(\n",
    "    x_dim='lon',\n",
    "    y_dim='lat',\n",
    "    inplace=True,\n",
    ").rio.write_coordinate_system(inplace=True)\n",
    "\n",
    "## set up the dataset transform we will use on every slice\n",
    "dst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n",
    "    src_crs=WGS84,\n",
    "    dst_crs=WGS84,\n",
    "    width=len(lon_EN4),\n",
    "    height=len(lat_EN4),\n",
    "    src_geoloc_array=(lon2d, lat2d),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef1dcb-60d1-4553-adff-b93be57b749e",
   "metadata": {},
   "source": [
    "Compute `coords` and `filtered_attrs` to be applied to each slice outside the loop, using a single depth slice.  Used 500 m in prototyping, but this choice shouldn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135932f-ae5d-4a03-a40b-12f7b4b2e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ex = test_ds_full['TF'].sel(depth=500, method='nearest').values\n",
    "## go straight to EN4\n",
    "## first value in np.full((t,y,x)) is length of the time dimension\n",
    "destination_ex = np.full((len(test_ds_full.time),\n",
    "                          dst_height, dst_width), np.nan)\n",
    "\n",
    "data_ex, transform_ex = rasterio.warp.reproject(\n",
    "    source_ex,\n",
    "    destination=destination_ex,\n",
    "    src_crs=WGS84,\n",
    "    dst_crs=WGS84,\n",
    "    dst_transform=dst_transform,\n",
    "    dst_nodata=np.nan, ## previously had the Verjans fill value here, but now that we've done\n",
    "    ## a `where` command to mask the dataset before depth resampling, the missing\n",
    "    ## values have been replaced by NaNs\n",
    "    src_geoloc_array=np.stack((lon2d, lat2d))\n",
    ")\n",
    "\n",
    "coords_ex = affine_to_coords(transform_ex, width=dst_width, height=dst_height, x_dim='lon', y_dim='lat')\n",
    "coords_ex.update(time=test_ds_full['time'])\n",
    "\n",
    "filtered_attrs = test_ds_full['TF'].attrs.copy()\n",
    "# filtered_attrs.pop(\"grid_mapping\", None) ## there is no grid_mapping written by default, don't need this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c143a-27d5-4ee8-90d6-d4b4a7e88528",
   "metadata": {},
   "source": [
    "#### Loop over depth levels to make rectilinear and reproject\n",
    "This may take a while, dataset is 14 GB.  In testing it took about 2 minutes on Lizz's NASA laptop.\n",
    "\n",
    "TODO: try implementing with Dask for greater efficiency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287e805-68fe-4395-bf18-f50220130351",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_slices = {}\n",
    "sim_slices = {}\n",
    "    \n",
    "for d in tobs_ds_full.depth.values[0:30]:\n",
    "    ## now start going rectilinear, then reproject_match\n",
    "    # sim_level = test_ds_full.sel(depth=d, method='nearest')\n",
    "    obs_level = tobs_ds_full.sel(depth=d)\n",
    "\n",
    "    ## get the sim_level on rectilinear grid\n",
    "    source = test_ds_full['TF'].sel(depth=d, method='nearest').values\n",
    "    destination = np.full((len(test_ds_full.time),\n",
    "                              dst_height, dst_width), np.nan)\n",
    "    \n",
    "    data, transform = rasterio.warp.reproject(\n",
    "        source,\n",
    "        destination=destination,\n",
    "        src_crs=WGS84,\n",
    "        dst_crs=WGS84,\n",
    "        dst_transform=dst_transform,\n",
    "        dst_nodata=np.nan, ## previously had the Verjans fill value here, but now that we've done\n",
    "        ## a `where` command to mask the dataset before depth resampling, the missing\n",
    "        ## values have been replaced by NaNs\n",
    "        src_geoloc_array=np.stack((lon2d, lat2d))\n",
    "    )\n",
    "    sim_da = xr.DataArray(data, coords=coords_ex, dims=('time', 'lat', 'lon'), name='TF', attrs=filtered_attrs)\n",
    "    sim_level = sim_da.where(sim_da<1.0e20) ## remove fill values\n",
    "    sim_level.rio.write_crs(WGS84, inplace=True).rio.set_spatial_dims(\n",
    "        x_dim='lon',\n",
    "        y_dim='lat',\n",
    "        inplace=True,\n",
    "    ).rio.write_coordinate_system(inplace=True)\n",
    "    \n",
    "    obs_repr = obs_level.rio.reproject_match(sim_level)\n",
    "    obs_slices[d] = obs_repr\n",
    "    # obs_slices[d] = {'dims': (\"time\", \"lat\", \"lon\"), 'data': obs_repr.data}\n",
    "    sim_slices[d] = sim_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edd442-0df8-400b-a6af-8f479667cafe",
   "metadata": {},
   "source": [
    "#### Concatenate to make nice dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa22aa-4b98-4395-bd7f-bddde0d05434",
   "metadata": {},
   "outputs": [],
   "source": [
    "obtemp = xr.concat([obs_slices[d] for d in tobs_ds_full.depth.values[0:30]], dim=tobs_ds_full.depth.values[0:30])\n",
    "obtemp = obtemp.drop_vars('depth') ## drop a 1D depth variable that carried through\n",
    "simtemp = xr.concat([sim_slices[d] for d in tobs_ds_full.depth.values[0:30]], dim=tobs_ds_full.depth.values[0:30])\n",
    "# simtemp = simtemp.drop_vars('depth') ## problem var is not present in sim\n",
    "\n",
    "tobs_repr_match = obtemp.rename({'concat_dim': 'depth', 'x': 'lon', 'y': 'lat'})\n",
    "tsim_match = simtemp.rename({'concat_dim': 'depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d166bf7-ce53-4cc2-b5ff-0f92a16a6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_repr_match = tobs_repr_match.to_dataset(name='TF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de21c2-cdfe-47dd-8f26-e7b1d07cbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm this is a Dataset\n",
    "tsim_match = tsim_match.to_dataset(name='TF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3620b8c-b7cc-4e94-997a-af670d7b93bc",
   "metadata": {},
   "source": [
    "This worked!  Try the QDM to make sure it will work as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766f585-1548-48fb-a77c-b56bf3fd0370",
   "metadata": {},
   "source": [
    "## Apply QDM correction\n",
    "We will apply the QDM correction on annual, de-trended series with the monthly residual variability re-applied afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c1bb71-3ee5-47a4-8afe-f9c49feeb761",
   "metadata": {},
   "source": [
    "### Separate annual and monthly var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f4e34-8bdc-48e8-be47-1fdc8a153e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_ds = tsim_match.resample(time='YS').mean()\n",
    "\n",
    "residual_ds = tsim_match.resample(time='ME').ffill() - annual_ds.resample(time='M').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee302a9a-777a-421b-b4ed-bbcccac21f33",
   "metadata": {},
   "source": [
    "### Detrend the data to be fit\n",
    "QDM performs poorly when values in the \"projection\" series exceed those seen in the \"historical\".  Detrend the future, historical, and reanalysis series before QDM correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838fc1e-69f9-4ca2-9f92-26aa4cdae6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_ser(da, dim, deg=1, var='tfdpavg0to500_bathymin100', return_fit=True):\n",
    "    ## based on Gist by Ryan Abernathey\n",
    "    ## hard coding Dataset version for now to make it behave with our datasets\n",
    "    \n",
    "    p = da.polyfit(dim=dim, deg=deg)\n",
    "    \n",
    "    if type(da) is xr.core.dataarray.DataArray:\n",
    "        fit=xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    elif type(da) is xr.core.dataset.Dataset:\n",
    "        fit = xr.polyval(da[dim], p.TF_polyfit_coefficients) \n",
    "        ## eventually use `var` argument to take any variable of interest\n",
    "        ## for now hard-coded the name of the depth-averaged thermal forcing, so this\n",
    "        ## will fail if we change that name\n",
    "    else:\n",
    "        print(\"Unrecognized input type. Expected xarray DataArray or Dataset, got {}\".format(type(da)))\n",
    "\n",
    "    if return_fit==True: ## give back the fitted values to use in reconstructing a series\n",
    "        return da-fit, fit\n",
    "    else:\n",
    "        return da-fit\n",
    "\n",
    "detrended_obs = detrend_ser(tobs_repr_match.sel(time=slice(BaselinePeriod[0],BaselinePeriod[1])).resample(time='A').mean(),\n",
    "                            dim='time',\n",
    "                            deg=1)[0]\n",
    "\n",
    "detrended_simh = detrend_ser(annual_ds.sel(time=slice(BaselinePeriod[0], BaselinePeriod[1])),\n",
    "                             dim='time',\n",
    "                             deg=1)[0]\n",
    "detrended_simp = detrend_ser(annual_ds.sel(time=slice(annual_ds.time[0], BaselinePeriod[0])),\n",
    "                             dim='time',\n",
    "                             deg=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4c94c-b217-41e1-a3a6-1d7efb6cba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## produce the QDM fit on detrended data\n",
    "qdm_detrended = apply_cmfunc(\n",
    "        method = \"quantile_delta_mapping\",\n",
    "        obs = detrended_obs,\n",
    "        simh = detrended_simh.rename({'time':'t_simh'}),\n",
    "        simp = detrended_simp,\n",
    "        n_quantiles = 100,\n",
    "        input_core_dims={\"obs\": \"time\", \"simh\": \"t_simh\", \"simp\": \"time\"},\n",
    "        kind = \"*\", # to calculate the relative rather than the absolute change, \"*\" can be used instead of \"+\" (this is prefered when adjusting precipitation)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff67b6e-12d4-474e-b377-822e67916dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reconstruct dataset from reanalysis mean val, future trend, QDM, and monthly residual\n",
    "reanalysis_meanval = tobs_repr_match.sel(time=slice(BaselinePeriod[0], BaselinePeriod[1])).mean(dim='time')\n",
    "# future_trendonly = (detrend_ser(annual_ds.sel(time=slice('1980', '2014')),\n",
    "#                              dim='time',\n",
    "#                              deg=1)[1]\n",
    "#                     - detrend_ser(annual_ds.sel(time=slice('1980', '2014')),\n",
    "#                              dim='time',\n",
    "#                              deg=1)[1])\n",
    "## Above is an apparent bug, subtracting two identical terms. Below is what *should* be the trend term\n",
    "## Corrected 17 Jul 2025\n",
    "hist_trendonly = detrend_ser(annual_ds.sel(time=slice(annual_ds.time[0], BaselinePeriod[0])),\n",
    "                             dim='time',\n",
    "                             deg=1)[1]\n",
    "# future_trend_series = reanalysis_meanval + future_trendonly\n",
    "hist_trend_series = reanalysis_meanval + hist_trendonly\n",
    "qdm_dtr_series = qdm_detrended.TF\n",
    "# qdm_plus_resid = future_trend_series.resample(time='ME').ffill() + qdm_dtr_series.resample(time='ME').ffill() + residual_ds.TF\n",
    "qdm_plus_resid = hist_trend_series.resample(time='ME').ffill() + qdm_dtr_series.resample(time='ME').ffill() + residual_ds.TF\n",
    "\n",
    "\n",
    "qdm_plus_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4b8d1-9e4f-4c13-b038-f8e606d9f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test write out\n",
    "from datetime import datetime, date\n",
    "\n",
    "now = datetime.now()\n",
    "# ds_temp = qdm_plus_resid.to_dataset(name='TF')\n",
    "ds_out = ds_temp.assign_attrs(title='QDM-corrected ocean thermal forcing for {}'.format(SelModel),\n",
    "                             summary='TF computed following Verjans code, in a bounding' + \n",
    "                              ' box around Greenland, for ISMIP7 Greenland forcing.' +\n",
    "                              ' QDM correction applied to annual based on Hadley data, with' +\n",
    "                              ' monthly residual added',\n",
    "                             institution='NASA Goddard Space Flight Center',\n",
    "                             creation_date=now.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924a14e-f645-4a4a-bda0-a21adeeedb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = DirSave + '/tfQDM-AllLevels-{}-{}_{}-{}.nc'.format(SelModel, \n",
    "                                                            YearsIncluded[0], YearsIncluded[1],\n",
    "                                                            date.today())\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    ds_out.to_netcdf(path=out_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
