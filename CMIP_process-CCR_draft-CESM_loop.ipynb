{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean thermal forcing -- Verjans refactored with xarray\n",
    "Clean ocean TF workflow for deployment on CCR.\n",
    "\n",
    "10 Oct 2024 | EHU\n",
    "- 15 Oct: try with CESM rather than IPSL, for now.  The [IPSL tripolar grid](https://cmc.ipsl.fr/international-projects/cmip5/ipsl-contribution-to-cmip5-faq/) is a complication to deal with in the next revision.\n",
    "- 16 Oct: CESM2 is successful (in 2000-2014 and 1950-1999 examples)! Next implement a for-loop.\n",
    "- 18 Oct: Metadata is not writing to output NetCDF on GHub...could be a problem with xarray version, but `xr.show_versions()` crashes the kernel.  So for now, we accept the metadata problem and attempt a full process of CESM files.\n",
    "- 23 Jun 25: Test process CESM-WACCM for test protocol\n",
    "- 7 Jul 25: Reprocess, correcting for depth expressed in cm rather than m in CESM-WACCM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import csv\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import dask\n",
    "from datetime import datetime, date\n",
    "\n",
    "from verjansFunctions import freezingPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to change the settings below, or modify them to work on CCR.  `DirThetaoNC` is where the `thetao` files are stored; `DirSoNC` is where `so` files are stored (could be the same directory!); and `out_path` is where to write the output NetCDF of this script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings for this run\n",
    "saveBoxGreenlandNC = True\n",
    "cwd                = os.getcwd()+'/'\n",
    "\n",
    "SelModel = 'CESM2-WACCM'\n",
    "\n",
    "# DirThetaoNC = f'/home/theghub/ehultee/projects/cmipdata/files/'\n",
    "DirThetaoNC = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/ISMIP/Summer25Test/'\n",
    "# DirSoNC     = f'/home/theghub/ehultee/projects/cmipdata/files/'\n",
    "DirSoNC = DirThetaoNC\n",
    "out_path = '/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/Summer25Test/'\n",
    "\n",
    "\n",
    "### Select experiment ###\n",
    "To2015hist                 = True\n",
    "To2100histssp585           = False\n",
    "To2100histssp126           = False\n",
    "\n",
    "# date_tags_to_run = ['1850', '1900', '1950', '2000'] ## stored separately in original method?\n",
    "date_tags_to_run = ['1850',] ## \n",
    "\n",
    "## Verjans stuff we shouldn't need\n",
    "## Could reconfigure to use these in labelling output files, if you're clever\n",
    "# if(To2015hist):\n",
    "#     Experiments = ['historical']\n",
    "#     DatesCut    = [2015]\n",
    "# elif(To2100histssp585): \n",
    "#     Experiments = ['historical','ssp585']\n",
    "#     DatesCut    = [2015,2100]\n",
    "# elif(To2100histssp126): \n",
    "#     Experiments = ['historical','ssp126']\n",
    "#     DatesCut    = [2015,2100]\n",
    "# nExp          = len(Experiments)\n",
    "# depthUnitConv = 1.0 #initialize depth unit converter\n",
    "\n",
    "### Limits of Greenland domain ###\n",
    "limN           = 86.0 ## degrees N latitude\n",
    "limS           = 57.0 ## degrees N latitude\n",
    "limE           = 4.0 ## degrees E latitude\n",
    "limW           = 274.0 ## degrees E latitude\n",
    "## CHECK: confirm that output shows up within this W-E box and not its E-W complement\n",
    "limDp          = 1200.0\n",
    "depthSubSample = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "nExp          = len(Experiments)\n",
    "depthUnitConv = 1.0 #initialize depth unit converter\n",
    "\n",
    "if(SelModel=='MIROCES2L'):\n",
    "    dim2d              = True\n",
    "    if(To2015hist):\n",
    "        ls_members     = [f'r{id}' for id in range(1,30+1)]\n",
    "    elif(To2100histssp585 or To2100histssp126):\n",
    "        ls_members     = [f'r{id}' for id in range(1,10+1)]\n",
    "    namelat            = 'latitude'\n",
    "    namelon            = 'longitude'\n",
    "    namez              = 'lev'\n",
    "    datesendhist       = np.array(['201412'])\n",
    "    if(To2100histssp585):\n",
    "        datesendssp585     = np.array(['210012'])\n",
    "    if(To2100histssp126):\n",
    "        datesendssp126     = np.array(['210012'])\n",
    "        \n",
    "if(SelModel=='IPSL-CM6A-LR'):\n",
    "    dim2d              = True\n",
    "    if(To2015hist):\n",
    "        ls_members     = [f'r{id}' for id in range(1,32+1)]\n",
    "        ls_members.remove('r2') #no r2 member for IPSLCM6A\n",
    "    elif(To2100histssp585 or To2100histssp126):\n",
    "        ls_members     = ['r1'] #,'r3','r4','r6','r14']\n",
    "    namelat            = 'nav_lat'\n",
    "    namelon            = 'nav_lon'\n",
    "    namez              = 'olevel'\n",
    "    datesRef           = [1850.0,2015.0,2040.0] \n",
    "    datesendhist       = np.array(['194912','201412'])\n",
    "    if(To2100histssp585):\n",
    "        datesendssp585     = np.array(['210012'])\n",
    "    if(To2100histssp126):\n",
    "        datesendssp126     = np.array(['210012'])\n",
    "\n",
    "else:\n",
    "    print(f'Error script not implemented yet for {SelModel}')\n",
    "\n",
    "# nMemb           = len(ls_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is legacy code from V. Verjans...we may want to update some of these hard-coded variable names for other GCMs eventually.  For now, it's going to tell you 'Error script not implemented yet', but it does not matter for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List the files to be read "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaFiles_test = []\n",
    "SoFiles_test = []\n",
    "for expt in Experiments:\n",
    "    fpath1 = DirThetaoNC+'thetao_Omon_{}_{}_'.format(SelModel, expt)\n",
    "    print(fpath1)\n",
    "    fpath2 = DirSoNC+'so_Omon_{}_{}_'.format(SelModel, expt)\n",
    "    th_temp = glob.glob(f'{fpath1}*.nc')\n",
    "    s_temp = glob.glob(f'{fpath2}*.nc')\n",
    "    ThetaFiles_test += th_temp ##concat the glob lists\n",
    "    SoFiles_test += s_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the list is not empty.  If it is, something has gone wrong in the directory access or in the generation of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaFiles_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, process, write out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the big processing.  These were split over several cells in my original script, but I am combining them here to allow a for-loop.  This might run into a Jupyter memory cap.  Let me know if you have problems and we can iterate on the best way to run this on your system.\n",
    "\n",
    "Specify the paths of the `thetao` and `so` variables -- ensure they come from the same GCM (`SelModel`) and time period.  Use a `with` statement to read in, trim, and close the parent datasets.  This should leave us with the trimmed datasets `gld_ds` and `gld_so` to work with below.\n",
    "\n",
    "Compute the ocean TF, mask grounded areas, and assign the result to an xarray Dataset.  Set metadata.\n",
    "\n",
    "Write the dataset out to a NetCDF file.  You may wish to change the `out_path` above, according to what you can access on CCR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for yt in date_tags_to_run:\n",
    "    path0 = [f for f in ThetaFiles_test if yt in f][0] ## thetao\n",
    "    path1 = [f for f in SoFiles_test if yt in f][0] ## salinity\n",
    "    ## we have to do a silly list comprehension with year tags, because the \n",
    "    ## glob lists are not stored in the same order\n",
    "    \n",
    "    ### Progress report\n",
    "    print('Running time period of {} on files\\n thetao: {} \\n salinity: {}\\n'.format(yt, path0, path1))\n",
    "\n",
    "    ###----------------\n",
    "    ### Load files\n",
    "    ###----------------\n",
    "    \n",
    "    ## load in and trim thetao\n",
    "    with xr.open_dataset(path0, chunks={'lev':10}) as ds:\n",
    "        ## trim to Greenland bounding box\n",
    "        include_lat = (ds.lat>=limS) & (ds.lat <=limN)\n",
    "        include_lon = np.logical_or(((ds.lon%360)<=limE),((ds.lon %360) >=limW)) \n",
    "        ## modulo 360 to account for lon going -180 to 180 or 0-360\n",
    "\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}): \n",
    "            ## mitigate performance problem with slicing\n",
    "            gld_ds = ds.where((include_lat & include_lon).compute(), drop=True)\n",
    "\n",
    "    ## load and trim so\n",
    "    with xr.open_dataset(path1, chunks={'lev':10}) as ds1:\n",
    "        ## trim to Greenland bounding box\n",
    "        include_lat = (ds1.lat>=limS) & (ds1.lat <=limN)\n",
    "        include_lon = np.logical_or(((ds1.lon%360)<=limE),((ds1.lon %360) >=limW))\n",
    "\n",
    "        with dask.config.set(**{'array.slicing.split_large_chunks': True}): \n",
    "            ## mitigate performance problem with slicing\n",
    "            gld_so = ds1.where((include_lat & include_lon).compute(), drop=True) ## trim to Gld\n",
    "    \n",
    "    ###----------------\n",
    "    ### Compute TF\n",
    "    ###----------------\n",
    "    fp = xr.apply_ufunc(freezingPoint, gld_so.so, gld_so.lev*0.01, dask='parallelized',\n",
    "                       dask_gufunc_kwargs={'allow_rechunk':True})\n",
    "    fftf = gld_ds.thetao - fp\n",
    "\n",
    "    ## mask and apply a fill value\n",
    "    tf_out = fftf.where(gld_ds.thetao<1e10, 1.1e20) ## apply Vincent's fill value of 1.1e20\n",
    "\n",
    "    tf_out = tf_out.assign_attrs(standard_name='TF',\n",
    "                        long_name='Ocean thermal forcing',\n",
    "                        fillvalue=1.1e20,\n",
    "                        latbounds=[limS, limN],\n",
    "                        lonbounds=[limW,limE])\n",
    "    \n",
    "    \n",
    "    ###----------------\n",
    "    ### Format dataset and metadata\n",
    "    ###----------------\n",
    "    now = datetime.now()\n",
    "    ds_temp = tf_out.to_dataset(name='TF')\n",
    "    ds_out = ds_temp.assign_attrs(title='Ocean thermal forcing for {}'.format(SelModel),\n",
    "                                 summary='TF computed following Verjans code, in a bounding' + \n",
    "                                  ' box around Greenland, for ISMIP7 Greenland forcing',\n",
    "                                 institution='NASA Goddard Space Flight Center',\n",
    "                                 creation_date=now.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    ###----------------\n",
    "    ### Write NetCDF out\n",
    "    ###----------------\n",
    "    year_tag = path0.strip('.nc').split('_')[-1] ## take the year tag from the GCM input (only one of the two input DS, but we have tried to make them match!)\n",
    "    out_fn = out_path + 'tf-{}-{}-{}.nc'.format(SelModel, year_tag, date.today())\n",
    "\n",
    "    from dask.diagnostics import ProgressBar\n",
    "\n",
    "    with ProgressBar():\n",
    "        ds_out.to_netcdf(path=out_fn)\n",
    "\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
