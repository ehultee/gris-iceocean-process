{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc68c4f1-a0f0-4a21-9406-0b98ca65e4af",
   "metadata": {},
   "source": [
    "# Step 3: Quantile Delta Mapping with reimplemented cmethods tools\n",
    "Apply reimplemented QDM functionality to Greenland thermal forcing. Streamline for applying on CCR.\n",
    "\n",
    "26 Mar 2025 | EHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5365b-cfc2-4f4a-8d24-2a904f491e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import cartopy.crs as ccrs ## map projections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rioxarray\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16b09b1-d74c-4bab-8fb9-0db25f61dbeb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## from cmethods.utils\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Optional, Union, TypeVar\n",
    "\n",
    "XRData_t = (xr.Dataset, xr.DataArray)\n",
    "NPData_t = (list, np.ndarray, np.generic)\n",
    "XRData = TypeVar(\"XRData\", xr.Dataset, xr.DataArray)\n",
    "NPData = TypeVar(\"NPData\", list, np.ndarray, np.generic)\n",
    "MAX_SCALING_FACTOR = 2 ## to allow multiplicative correction?\n",
    "\n",
    "\n",
    "def check_xr_types(obs: XRData, simh: XRData, simp: XRData) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type xarray.core.dataarray.Dataset or xarray.core.dataarray.DataArray\"\n",
    "\n",
    "    if not isinstance(obs, XRData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, XRData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, XRData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def check_np_types(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type list, np.ndarray or np.generic\"\n",
    "\n",
    "    if not isinstance(obs, NPData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, NPData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, NPData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def nan_or_equal(value1: float, value2: float) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the values are equal or at least one is NaN\n",
    "\n",
    "    :param value1: First value to check\n",
    "    :type value1: float\n",
    "    :param value2: Second value to check\n",
    "    :type value2: float\n",
    "    :return: If any value is NaN or values are equal\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    return np.isnan(value1) or np.isnan(value2) or value1 == value2\n",
    "        \n",
    "def ensure_dividable(\n",
    "    numerator: Union[float, np.ndarray],\n",
    "    denominator: Union[float, np.ndarray],\n",
    "    max_scaling_factor: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensures that the arrays can be divided. The numerator will be multiplied by\n",
    "    the maximum scaling factor of the CMethods class if division by zero.\n",
    "\n",
    "    :param numerator: Numerator to use\n",
    "    :type numerator: np.ndarray\n",
    "    :param denominator: Denominator that can be zero\n",
    "    :type denominator: np.ndarray\n",
    "    :return: Zero-ensured division\n",
    "    :rtype: np.ndarray | float\n",
    "    \"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        result = numerator / denominator\n",
    "\n",
    "    if isinstance(numerator, np.ndarray):\n",
    "        mask_inf = np.isinf(result)\n",
    "        result[mask_inf] = numerator[mask_inf] * max_scaling_factor  # type: ignore[index]\n",
    "\n",
    "        mask_nan = np.isnan(result)\n",
    "        result[mask_nan] = 0  # type: ignore[index]\n",
    "    elif np.isinf(result):\n",
    "        result = numerator * max_scaling_factor\n",
    "    elif np.isnan(result):\n",
    "        result = 0.0\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_pdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Compuites and returns the the probability density function :math:`P(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: The vector to get :math:`P(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`P(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The probability densitiy function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the probability density function :math:`P(x)`\n",
    "\n",
    "        >>> from cmethods get_pdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_pdf(x=x, xbins=xbins))\n",
    "        [2, 5, 5]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def get_cdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Computes and returns returns the cumulative distribution function :math:`F(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: Vector to get :math:`F(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`F(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The cumulative distribution function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the cumulative distribution function :math:`F(x)`\n",
    "\n",
    "        >>> from cmethods.utils import get_cdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_cdf(x=x, xbins=xbins))\n",
    "        [0.0, 0.16666667, 0.58333333, 1.]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    cdf = np.insert(np.cumsum(pdf), 0, 0.0)\n",
    "    return cdf / cdf[-1]\n",
    "\n",
    "\n",
    "def get_inverse_of_cdf(\n",
    "    base_cdf: Union[list, np.ndarray],\n",
    "    insert_cdf: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Returns the inverse cumulative distribution function as:\n",
    "    :math:`F^{-1}_{x}\\left[y\\right]` where :math:`x` represents ``base_cdf`` and\n",
    "    ``insert_cdf`` is represented by :math:`y`.\n",
    "\n",
    "    :param base_cdf: The basis\n",
    "    :type base_cdf: list | np.ndarray\n",
    "    :param insert_cdf: The CDF that gets inserted\n",
    "    :type insert_cdf: list | np.ndarray\n",
    "    :param xbins: Probability boundaries\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The inverse CDF\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    return np.interp(insert_cdf, base_cdf, xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc0d0be-6fc7-4aed-b402-7cbb2fc09c36",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def quantile_delta_mapping(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    "    n_quantiles: int,\n",
    "    kind: str = \"+\",\n",
    "    **kwargs,\n",
    "    ) -> NPData:\n",
    "    r\"\"\"\n",
    "    Based on https://python-cmethods.readthedocs.io/en/latest/methods.html#quantile-delta-mapping\n",
    "\n",
    "    kind: str, default + for additive, can be set to * for multiplicative\n",
    "    \"\"\"\n",
    "    # check_adjust_called(\n",
    "    #     function_name=\"quantile_delta_mapping\",\n",
    "    #     adjust_called=kwargs.get(\"adjust_called\"),\n",
    "    # )\n",
    "    check_np_types(obs=obs, simh=simh, simp=simp)\n",
    "\n",
    "    if not isinstance(n_quantiles, int):\n",
    "        raise TypeError(\"'n_quantiles' must be type int\")\n",
    "\n",
    "    if kind=='+':\n",
    "        obs, simh, simp = (\n",
    "            np.array(obs),\n",
    "            np.array(simh),\n",
    "            np.array(simp),\n",
    "        )  # to achieve higher accuracy\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", min(np.nanmin(obs), np.nanmin(simh)))\n",
    "\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = abs(global_max - global_min) / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        # calculate exact CDF values of $F_{sim,p}[T_{sim,p}(t)]$\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "        delta = simp - get_inverse_of_cdf(cdf_simh, epsilon, xbins)  # Eq. 1.3\n",
    "        return QDM1 + delta  # Eq. 1.4\n",
    "\n",
    "    if kind=='*':\n",
    "        obs, simh, simp = np.array(obs), np.array(simh), np.array(simp)\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", 0.0)\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = global_max / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "\n",
    "        delta = ensure_dividable(  # Eq. 2.3\n",
    "            simp,\n",
    "            get_inverse_of_cdf(cdf_simh, epsilon, xbins),\n",
    "            max_scaling_factor=kwargs.get(\n",
    "                \"max_scaling_scaling\",\n",
    "                MAX_SCALING_FACTOR,\n",
    "            ),\n",
    "        )\n",
    "        return QDM1 * delta  # Eq. 2.4\n",
    "    raise NotImplementedError(\n",
    "        f\"{kind=} not available for quantile_delta_mapping. Use '+' or '*' instead.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_cmfunc(\n",
    "    method: str,\n",
    "    obs: XRData,\n",
    "    simh: XRData,\n",
    "    simp: XRData,\n",
    "    **kwargs: dict,\n",
    ") -> XRData:\n",
    "    \"\"\"\n",
    "    Internal function used to apply the bias correction technique to the\n",
    "    passed input data.\n",
    "    \"\"\"\n",
    "    ## hard-code the QDM method\n",
    "    if method!='quantile_delta_mapping':\n",
    "        raise UnknownMethodError('Not implemented for methods other than quantile_delta_mapping')\n",
    "        ## give this a default for what we want to do\n",
    "    else:\n",
    "        method='quantile_delta_mapping' ## not actually going to use this\n",
    "    \n",
    "    check_xr_types(obs=obs, simh=simh, simp=simp)\n",
    "    # if method not in __METHODS_FUNC__:\n",
    "    #     raise UnknownMethodError(method, __METHODS_FUNC__.keys())\n",
    "\n",
    "    if kwargs.get(\"input_core_dims\"):\n",
    "        if not isinstance(kwargs[\"input_core_dims\"], dict):\n",
    "            raise TypeError(\"input_core_dims must be an object of type 'dict'\")\n",
    "        if not len(kwargs[\"input_core_dims\"]) == 3 or any(\n",
    "            not isinstance(value, str) for value in kwargs[\"input_core_dims\"].values()\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                'input_core_dims must have three key-value pairs like: {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}',\n",
    "            )\n",
    "\n",
    "        input_core_dims = kwargs.pop(\"input_core_dims\")\n",
    "    else:\n",
    "        input_core_dims = {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}\n",
    "\n",
    "    result: XRData = xr.apply_ufunc(\n",
    "        quantile_delta_mapping,\n",
    "        obs,\n",
    "        simh,\n",
    "        # Need to spoof a fake time axis since 'time' coord on full dataset is\n",
    "        # different than 'time' coord on training dataset.\n",
    "        simp.rename({input_core_dims[\"simp\"]: \"__t_simp__\"}),\n",
    "        dask=\"parallelized\",\n",
    "        vectorize=True,\n",
    "        # This will vectorize over the time dimension, so will submit each grid\n",
    "        # cell independently\n",
    "        input_core_dims=[\n",
    "            [input_core_dims[\"obs\"]],\n",
    "            [input_core_dims[\"simh\"]],\n",
    "            [\"__t_simp__\"],\n",
    "        ],\n",
    "        # Need to denote that the final output dataset will be labeled with the\n",
    "        # spoofed time coordinate\n",
    "        output_core_dims=[[\"__t_simp__\"]],\n",
    "        kwargs=dict(kwargs),\n",
    "    )\n",
    "\n",
    "    # Rename to proper coordinate name.\n",
    "    result = result.rename({\"__t_simp__\": input_core_dims[\"simp\"]})\n",
    "\n",
    "    # ufunc will put the core dimension to the end (time), so want to preserve\n",
    "    # original order where time is commonly first.\n",
    "    return result.transpose(*obs.rename({input_core_dims[\"obs\"]: input_core_dims[\"simp\"]}).dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad2805-27da-4142-97e3-c05503363ca8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Time utils from Bryan Riel\n",
    "## pasting stuff from iceutils below.\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "def tdec2datestr(tdec_in, returndate=False):\n",
    "    \"\"\"\n",
    "    Convert a decimaly year to an iso date string.\n",
    "    \"\"\"\n",
    "    if isinstance(tdec_in, (list, np.ndarray)):\n",
    "        tdec_list = copy.deepcopy(tdec_in)\n",
    "    else:\n",
    "        tdec_list = [tdec_in]\n",
    "    current_list = []\n",
    "    for tdec in tdec_list:\n",
    "        year = int(tdec)\n",
    "        yearStart = datetime.datetime(year, 1, 1)\n",
    "        if year % 4 == 0:\n",
    "            ndays_in_year = 366.0\n",
    "        else:\n",
    "            ndays_in_year = 365.0\n",
    "        days = (tdec - year) * ndays_in_year\n",
    "        seconds = (days - int(days)) * 86400\n",
    "        tdelta = datetime.timedelta(days=int(days), seconds=int(seconds))\n",
    "        current = yearStart + tdelta\n",
    "        if not returndate:\n",
    "            current = current.isoformat(' ').split()[0]\n",
    "        current_list.append(current)\n",
    "\n",
    "    if len(current_list) == 1:\n",
    "        return current_list[0]\n",
    "    else:\n",
    "        return np.array(current_list)\n",
    "\n",
    "\n",
    "def datestr2tdec(yy=0, mm=0, dd=0, hour=0, minute=0, sec=0, microsec=0, dateobj=None):\n",
    "    \"\"\"\n",
    "    Convert year, month, day, hours, minutes, seconds to decimal year.\n",
    "    \"\"\"\n",
    "    if dateobj is not None:\n",
    "        if type(dateobj) == str:\n",
    "            yy, mm, dd = [int(val) for val in dateobj.split('-')]\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        elif type(dateobj) == datetime.datetime:\n",
    "            attrs = ['year', 'month', 'day', 'hour', 'minute', 'second']\n",
    "            yy, mm, dd, hour, minute, sec = [getattr(dateobj, attr) for attr in attrs]\n",
    "        elif type(dateobj) == np.datetime64:\n",
    "            yy = dateobj.astype('datetime64[Y]').astype(int) + 1970\n",
    "            mm = dateobj.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "            days = (\n",
    "                (dateobj - dateobj.astype('datetime64[M]')) / np.timedelta64(1, 'D')\n",
    "            )\n",
    "            dd = int(days) + 1\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        else:\n",
    "            raise NotImplementedError('dateobj must be str, datetime, or np.datetime64.')\n",
    "\n",
    "    # Make datetime object for start of year\n",
    "    yearStart = datetime.datetime(yy, 1, 1, 0, 0, 0)\n",
    "    # Make datetime object for input time\n",
    "    current = datetime.datetime(yy, mm, dd, hour, minute, sec, microsec)\n",
    "    # Compute number of days elapsed since start of year\n",
    "    tdelta = current - yearStart\n",
    "    # Convert to decimal year and account for leap year\n",
    "    if yy % 4 == 0:\n",
    "        return float(yy) + tdelta.total_seconds() / (366.0 * 86400)\n",
    "    else:\n",
    "        return float(yy) + tdelta.total_seconds() / (365.0 * 86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975be2d2-d79d-4b42-9325-d455666e1d2e",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dad75b-f34b-4a83-bd9f-7e2196c95fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DepthRange         = [0,500]\n",
    "ShallowThreshold   = 100\n",
    "PeriodObs0         = [1950,2015]\n",
    "SelModel = 'CESM'\n",
    "\n",
    "DirSave = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles'\n",
    "DirEN4         = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Documents/ISMIP7/Verjans-process/'\n",
    "EN4file        = f'dpavg_tf_EN4anl_Dp{DepthRange[0]}to{DepthRange[1]}_bathymin{ShallowThreshold}.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dab0cf7-70ab-4e7e-b387-10839be02a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load EN4 using xarray\n",
    "ds = xr.open_dataset(DirEN4+EN4file, decode_times='timeDim')\n",
    "ds2 = ds.assign_coords({'timeDim': ds.time, \n",
    "                  'latDim': ds.lat, \n",
    "                  'lonDim': ds.lon,\n",
    "                  'depthDim': ds.depth})\n",
    "\n",
    "tfEN4 = ds2.tfdpavg0to500_bathymin100.rename({'timeDim': 'time',\n",
    "                                              'latDim': 'lat',\n",
    "                                              'lonDim': 'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b659f2-2a06-4e3f-9bc8-db56b3c00e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in CESM TF for all time slices available, using multifile dataset\n",
    "with xr.open_mfdataset(f'{DirEN4}/tfdpavg*.nc') as ds: \n",
    "    ds3 = ds.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22493b2f-729d-4386-9d5a-0b51be3e691b",
   "metadata": {},
   "source": [
    "### Process grids to match\n",
    "Because the grids are offset by 0.5° from each other, we will need to warp/resample using `rioxarray` before we run the QDM correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180b978-d7ed-4119-b9f5-2717422b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_full = ds3.TF\n",
    "\n",
    "## aligning the time indices\n",
    "test_ds_full = test_ds_full.assign_coords(new_time = ('time', test_ds_full.indexes['time'].to_datetimeindex().values))\n",
    "test_ds_full = test_ds_full.drop_indexes('time')\n",
    "test_ds_full = test_ds_full.set_xindex('new_time').drop_vars('time')\n",
    "\n",
    "## aligning the names of the variables between obs and sim\n",
    "test_ds_full = test_ds_full.to_dataset()\n",
    "test_ds_full = test_ds_full.rename({'new_time': 'time', 'TF': 'tfdpavg0to500_bathymin100'})\n",
    "test_ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54f475-c63a-491e-9a70-a886609afd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tobs_ds_full = tfEN4.sel( \n",
    "                    time=slice('1950', '2015'))\n",
    "\n",
    "tobs_ds_full = tobs_ds_full.assign_coords(new_time = ('time', pd.to_datetime(tdec2datestr(tobs_ds_full.time.values))))\n",
    "tobs_ds_full = tobs_ds_full.drop_indexes('time')\n",
    "tobs_ds_full = tobs_ds_full.set_xindex('new_time').drop_vars('time')\n",
    "tobs_ds_full = tobs_ds_full.rename({'new_time': 'time'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840272d-38ae-4a7f-89df-87e14a6cdee4",
   "metadata": {},
   "source": [
    "### Reproject obs to match CESM grid\n",
    "Warp the offset grids to match using `rioxarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dd3134-e834-4fe7-81f1-aa17ae4a2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CRS(\"EPSG:4326\")\n",
    "\n",
    "tobs_ds_full.rio.write_crs(cc, inplace=True).rio.set_spatial_dims(\n",
    "    x_dim=\"lon\",\n",
    "    y_dim=\"lat\",\n",
    "    inplace=True,\n",
    ").rio.write_coordinate_system(inplace=True)\n",
    "\n",
    "test_ds_full.rio.write_crs(cc, inplace=True).rio.set_spatial_dims(\n",
    "    x_dim=\"lon\",\n",
    "    y_dim=\"lat\",\n",
    "    inplace=True,\n",
    ").rio.write_coordinate_system(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981fb676-47c6-4274-aea4-84062a8d87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_repr_match = tobs_ds_full.rio.reproject_match(test_ds_full)\n",
    "\n",
    "## rename the coords \n",
    "tobs_repr_match = tobs_repr_match.rename({'x': 'lon', 'y': 'lat'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096eb6cc-efed-4ce5-80f6-8f17075dee72",
   "metadata": {},
   "source": [
    "## Apply QDM correction\n",
    "We will apply the QDM correction on annual, de-trended series with the monthly residual variability re-applied afterward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac572f-06a9-4318-a88c-bc4bc8eddc97",
   "metadata": {},
   "source": [
    "### Separate annual and monthly var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f4e34-8bdc-48e8-be47-1fdc8a153e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_ds = test_ds_full.resample(time='YS').mean()\n",
    "\n",
    "residual_ds = test_ds_full.resample(time='ME').ffill() - annual_ds.resample(time='M').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e215e5-95c0-4398-b916-aa63a7ad4f76",
   "metadata": {},
   "source": [
    "### Detrend the data to be fit\n",
    "QDM performs poorly when values in the \"projection\" series exceed those seen in the \"historical\".  Detrend the future, historical, and reanalysis series before QDM correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838fc1e-69f9-4ca2-9f92-26aa4cdae6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detrend_ser(da, dim, deg=1, var='tfdpavg0to500_bathymin100', return_fit=True):\n",
    "    ## based on Gist by Ryan Abernathey\n",
    "    ## hard coding Dataset version for now to make it behave with our datasets\n",
    "    \n",
    "    p = da.polyfit(dim=dim, deg=deg)\n",
    "    \n",
    "    if type(da) is xr.core.dataarray.DataArray:\n",
    "        fit=xr.polyval(da[dim], p.polyfit_coefficients)\n",
    "    elif type(da) is xr.core.dataset.Dataset:\n",
    "        fit = xr.polyval(da[dim], p.tfdpavg0to500_bathymin100_polyfit_coefficients) \n",
    "        ## eventually use `var` argument to take any variable of interest\n",
    "        ## for now hard-coded the name of the depth-averaged thermal forcing, so this\n",
    "        ## will fail if we change that name\n",
    "    else:\n",
    "        print(\"Unrecognized input type. Expected xarray DataArray or Dataset, got {}\".format(type(da)))\n",
    "\n",
    "    if return_fit==True: ## give back the fitted values to use in reconstructing a series\n",
    "        return da-fit, fit\n",
    "    else:\n",
    "        return da-fit\n",
    "\n",
    "detrended_obs = detrend_ser(tobs_repr_match.sel(time=slice('1950','1980')).resample(time='A').mean(),\n",
    "                            dim='time',\n",
    "                            deg=1)[0]\n",
    "\n",
    "detrended_simh = detrend_ser(annual_ds.sel(time=slice('1950', '1980')),\n",
    "                             dim='time',\n",
    "                             deg=1)[0]\n",
    "detrended_simp = detrend_ser(annual_ds.sel(time=slice('1980', '2014')),\n",
    "                             dim='time',\n",
    "                             deg=1)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4c94c-b217-41e1-a3a6-1d7efb6cba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## produce the QDM fit on detrended data\n",
    "qdm_detrended = apply_cmfunc(\n",
    "        method = \"quantile_delta_mapping\",\n",
    "        obs = detrended_obs,\n",
    "        simh = detrended_simh.rename({'time':'t_simh'}),\n",
    "        simp = detrended_simp,\n",
    "        n_quantiles = 100,\n",
    "        input_core_dims={\"obs\": \"time\", \"simh\": \"t_simh\", \"simp\": \"time\"},\n",
    "        kind = \"*\", # to calculate the relative rather than the absolute change, \"*\" can be used instead of \"+\" (this is prefered when adjusting precipitation)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6d17dd-0f25-4ca1-b986-2aa3b24559cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## get obs baseline to add to QDM-corrected annual variability\n",
    "\n",
    "## set degree and dimension for polyfit\n",
    "detrend_dim = 'time'\n",
    "detrend_deg = 1\n",
    "\n",
    "obs_pf = tobs_repr_match.sel(time=slice('1950','1980')).resample(time='A').mean().polyfit(\n",
    "    dim=detrend_dim, deg=detrend_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff67b6e-12d4-474e-b377-822e67916dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reconstruct dataset from reanalysis mean val, future trend, QDM, and monthly residual\n",
    "reanalysis_meanval = obs_pf.polyfit_coefficients.sel(degree=0)\n",
    "future_trendonly = (detrend_ser(annual_ds.sel(time=slice('1980', '2014')),\n",
    "                             dim='time',\n",
    "                             deg=1)[1]\n",
    "                    - detrend_ser(annual_ds.sel(time=slice('1980', '2014')),\n",
    "                             dim='time',\n",
    "                             deg=1)[1])\n",
    "future_trend_series = reanalysis_meanval + future_trendonly\n",
    "qdm_dtr_series = qdm_detrended.tfdpavg0to500_bathymin100\n",
    "qdm_plus_resid = future_trend_series.resample(time='ME').ffill() + qdm_dtr_series.resample(time='ME').ffill() + residual_ds.tfdpavg0to500_bathymin100\n",
    "\n",
    "qdm_plus_resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f4b8d1-9e4f-4c13-b038-f8e606d9f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test write out\n",
    "from datetime import datetime, date\n",
    "\n",
    "now = datetime.now()\n",
    "ds_temp = qdm_plus_resid.to_dataset(name='TF')\n",
    "# ds_temp.TF.attrs = tf_out.attrs\n",
    "ds_out = ds_temp.assign_attrs(title='QDM-corrected ocean thermal forcing for {}'.format(SelModel),\n",
    "                             summary='TF computed following Verjans code, in a bounding' + \n",
    "                              ' box around Greenland, for ISMIP7 Greenland forcing.' +\n",
    "                              ' QDM correction applied to annual based on EN4 data, with' +\n",
    "                              ' monthly residual added',\n",
    "                             institution='NASA Goddard Space Flight Center',\n",
    "                             creation_date=now.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924a14e-f645-4a4a-bda0-a21adeeedb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = DirSave + '/tfQDM-{}-{}.nc'.format(SelModel, date.today())\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    ds_out.to_netcdf(path=out_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
