{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ocean thermal forcing from ORAS5 data\n",
    "Clean ocean TF workflow to process ORAS5 reanalysis data.\n",
    "\n",
    "21 Apr 2025 | EHU\n",
    "- Note that dataset is very large (~350 GB) due to high resolution of ORAS model.  Attempt to load in lat/lon subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import copy\n",
    "import csv\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import dask\n",
    "from datetime import datetime\n",
    "\n",
    "from verjansFunctions import freezingPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings for this run\n",
    "saveBoxGreenlandNC = True\n",
    "cwd                = os.getcwd()+'/'\n",
    "\n",
    "SelModel = 'ORAS5'\n",
    "\n",
    "data_directory = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/Ocean-reanalyses/'+SelModel\n",
    "DirSaveNC   = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/'\n",
    "\n",
    "\n",
    "### Limits of Greenland domain ###\n",
    "limN           = 86.0 ## degrees N latitude\n",
    "limS           = 57.0 ## degrees N latitude\n",
    "limE           = 4.0 ## degrees E latitude\n",
    "limW           = 274.0 ## degrees E latitude\n",
    "## CHECK: confirm that output shows up within this W-E box and not its E-W complement\n",
    "limDp          = 1200.0\n",
    "depthSubSample = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and trim data.\n",
    "Load in from multiple files. ORAS5 will be too big to load in fully before trimming.  Also, potential temp and salinity come in different datasets. Custom read-in command for this use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all tiles together using multifile dataset\n",
    "## trim before loading in to memory\n",
    "\n",
    "with xr.open_mfdataset(f'{data_directory}/votemper*.nc') as ds_temp: \n",
    "    ## trim to Greenland bounding box\n",
    "    include_lat = (ds_temp.nav_lat>=limS) & (ds_temp.nav_lat <=limN)\n",
    "    include_lon = np.logical_or(((ds_temp.nav_lon%360)<=limE),((ds_temp.nav_lon %360) >=limW)) \n",
    "    ## modulo 360 to account for lon going -180 to 180 or 0-360\n",
    "    gld_pt = ds_temp.where((include_lat & include_lon).compute(), drop=True)\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}): ## mitigate performance problem with slicing\n",
    "        gld_ds_pt = gld_pt.load()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gld_ds_pt.votemper.mean() ## check it's reasonable and non-NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_mfdataset(f'{data_directory}/vosaline*.nc') as ds_temp: \n",
    "    ## trim to Greenland bounding box\n",
    "    include_lat = (ds_temp.nav_lat>=limS) & (ds_temp.nav_lat <=limN)\n",
    "    include_lon = np.logical_or(((ds_temp.nav_lon%360)<=limE),((ds_temp.nav_lon %360) >=limW)) \n",
    "    ## modulo 360 to account for lon going -180 to 180 or 0-360\n",
    "    gld_sal = ds_temp.where((include_lat & include_lon).compute(), drop=True)\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': True}): ## mitigate performance problem with slicing\n",
    "        gld_ds_sal = gld_sal.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gld_ds_sal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the ocean thermal forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## This process for ORA5\n",
    "fp = xr.apply_ufunc(freezingPoint, gld_ds_sal.vosaline, gld_ds_sal.deptht, dask='parallelized',\n",
    "                    dask_gufunc_kwargs={'allow_rechunk':True})\n",
    "fftf = gld_ds_pt.votemper - fp ## already in Celsius\n",
    "fftf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for write-out\n",
    "Mask dataset to remove data missing from original.  Rename unconventionally-named dimensions.  Assign metadata to write to NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mask and apply a fill value\n",
    "tf_out = fftf.where(gld_ds_pt.votemper<1e10) ## let xarray do its native processing with NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_out = tf_out.rename({'deptht':'depth', \n",
    "                        'nav_lon':'lon', \n",
    "                        'nav_lat':'lat', \n",
    "                        'time_counter':'time'})\n",
    "tf_out.assign_attrs(standard_name='TF',\n",
    "                    long_name='Ocean thermal forcing',\n",
    "                    # fillvalue=1.1e20,\n",
    "                    latbounds=[limS, limN],\n",
    "                    lonbounds=[limW,limE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "ds_temp = tf_out.to_dataset(name='TF')\n",
    "\n",
    "ds_out = ds_temp.assign_attrs(title='Ocean thermal forcing for {}'.format(SelModel),\n",
    "                             summary='TF computed following Verjans code, in a bounding' + \n",
    "                              ' box around Greenland, for ISMIP7 Greenland forcing.' +\n",
    "                              ' This version for {}'.format(SelModel),\n",
    "                             institution='NASA Goddard Space Flight Center',\n",
    "                             creation_date=now.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write NetCDF out\n",
    "Write to a custom filename in the directory specified above.  Remember to rename the file as needed, e.g. for the correct date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_fn = DirSaveNC + 'tf-{}-1990_2005.nc'.format(SelModel)\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "with ProgressBar():\n",
    "    ds_out.to_netcdf(path=out_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy  # Map projections libary\n",
    "import cartopy.crs as ccrs  # Projections list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_in = out_fn\n",
    "\n",
    "ds_new = xr.open_dataset(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tavg = ds_new.TF.mean(dim='time') \n",
    "tf_tavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_tavg.sel(depth=5.02, method='nearest').mean(skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "tf_tavg.sel(depth=200, method='nearest').plot(ax=ax, transform=ccrs.PlateCarree(), x='lon', y='lat') ## specify x and y coordinates\n",
    "ax.coastlines(); ax.gridlines();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_depth = 100\n",
    "fig, ax = plt.subplots()\n",
    "tf_tavg.sel(depth=demo_depth, method='nearest').plot(ax=ax)\n",
    "ax.set(title='TF at {} m depth, {}, average 1990-2005'.format(demo_depth, SelModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
