{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d505d56-5967-44b4-8401-6d4673716977",
   "metadata": {},
   "source": [
    "# Extrapolate bias-corrected TF from EN4 grid to ISMIP\n",
    "Based on Donald's code for extrapolating ocean properties.  Accounts for effective depth using lookup table.\n",
    "\n",
    "21 Jul 2025 | EHU\n",
    "- 24 Jul: Attempt with xarray functionality, as NetCDF complaining about shape of grid.  Tried to convert coords on a single slice, but this did not work.  Try making meshgrid?  Or try CESM test case instead of EN4?\n",
    "- 25 Jul: Meshgrid worked for EN4.  Adapted for use with CESM2-WACCM test file, which already has 2D lat/lon, and this worked well with original pyproj functionality.  Confirmed that mapped values at least look like Greenland area.\n",
    "- 28 Jul: fixed later part of code that maps to ISMIP grid. Dimensions were out of order.\n",
    "- 30 Jul: apply in full process of CESM2-WACCM data, 1850-2014.  Use output of Step2 notebook (QDM corrected)\n",
    "- 31 Jul: apply to additional QDM-corrected CESM2-WACCM output, 1985-2100.\n",
    "- 19 Sept: apply to common-gridded QDM output, 1850-2100.\n",
    "- 24 Sept: Attempt setting TF<0 to 0 and taking no other mask/fill action.  This produces apparently complete output.  Yay!  Now attempting to process EN4, CESM2-WACCM uncorrected, and mean-corrected CESM2-WACCM fields to the ISMIP grid.\n",
    "- 25 Sept: Introduce batch writing for long series. Use `cftime.DatetimeNoLeap` to encode time axes, since 2101-2300 series will extend beyond end of `datetime64[ns]` calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce8975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import Dataset\n",
    "import os\n",
    "from pyproj import Transformer\n",
    "from scipy.interpolate import interp1d\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2c56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load effective geometry (spatial look-up table)\n",
    "\n",
    "# files\n",
    "xy_eff_file = '/Users/eultee/Library/CloudStorage/OneDrive-NASA/Documents/ISMIP7/Slater-extrapolate/XY_eff.nc'\n",
    "z_eff_file = '/Users/eultee/Library/CloudStorage/OneDrive-NASA/Documents/ISMIP7/Slater-extrapolate/z_eff.nc'\n",
    "\n",
    "# effective geometry\n",
    "# NB replace masked values with NaNs\n",
    "X_eff = nc.Dataset(xy_eff_file).variables['X_eff'][:].filled(np.nan)\n",
    "Y_eff = nc.Dataset(xy_eff_file).variables['Y_eff'][:].filled(np.nan)\n",
    "z_eff = nc.Dataset(z_eff_file).variables['z_eff'][:].filled(np.nan)\n",
    "\n",
    "# ismip coordinates at which effective geometry applies\n",
    "x = nc.Dataset(xy_eff_file).variables['x'][:].filled(np.nan)\n",
    "y = nc.Dataset(xy_eff_file).variables['y'][:].filled(np.nan)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# vertical grid of effective depths\n",
    "z = np.flipud(np.unique(z_eff[z_eff<=0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34c60e9-a6ef-4fdf-a693-caeab4224fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots()\n",
    "sc = ax.scatter(X_eff[::4, ::4], Y_eff[::4, ::4], c=z_eff[::4,::4])\n",
    "plt.colorbar(sc, label='effective depth read in (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4334b4df-5dda-4986-a601-d19389f53070",
   "metadata": {},
   "source": [
    "Use xarray to load in a view of the dataset for future reference.\n",
    "\n",
    "`rioxarray` options not used here, but may help other use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541387e5-2668-4285-bf30-6732cee3b5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load and process data\n",
    "import xarray\n",
    "# import rioxarray\n",
    "# import rasterio\n",
    "\n",
    "## files\n",
    "SelModel = 'CESM2-WACCM' ## model label for input and output\n",
    "PressureAlreadyIncluded = True ## was the pressure effect accounted for in Step 1?\n",
    "YearTags = ['1985', '2101']\n",
    "\n",
    "DirSave = f'/Users/eultee/Desktop/'\n",
    "\n",
    "model_path = DirSave\n",
    "# if PressureAlreadyIncluded:\n",
    "#     model_path = model_path + 'Default_FullProcess/'\n",
    "#     model_file = model_path + 'tfQDM-AllLevels-{}-{}_{}-IncludingPressure-20250731.nc'.format(SelModel, YearTags[0], YearTags[1])\n",
    "# else:\n",
    "#     model_path = model_path + 'FP_in_Step3/'\n",
    "#     model_file = model_path + 'tfQDM-AllLevels-{}-1850_2014-NoPressure-2025-07-30.nc'.format(SelModel)\n",
    "\n",
    "model_file = f'{DirSave}/tf_MeanCorrected-AllLevels-CommonGrid-CESM2-WACCM-1850_2100-IncludingPressure-20250924.nc'\n",
    "ds_model = xr.open_dataset(model_file, decode_times='timeDim')\n",
    "\n",
    "# ds_model = xr.open_mfdataset(f'{DirSave}/TF-PressureIncluded/TF_Omon_CESM2*.nc').sel(time=slice(YearsIncluded[0], YearsIncluded[1]))\n",
    "\n",
    "\n",
    "# ds_slice = ds_en4.sel(depth=500, method='nearest')\n",
    "\n",
    "# WGS84 = rasterio.crs.CRS.from_epsg(4326)\n",
    "# ds_slice.rio.write_crs(WGS84, inplace=True).rio.set_spatial_dims(\n",
    "#     x_dim='lon',\n",
    "#     y_dim='lat',\n",
    "#     inplace=True,\n",
    "# ).rio.write_coordinate_system(inplace=True)\n",
    "\n",
    "# ds_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4cdc6-3e65-4a5d-b382-c09ecc66c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f54fae-47e0-4caa-bfbf-84205298c124",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_model.sel(depth=500, method='nearest').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1252ec5c-619c-4fc8-b424-9e6ae55d4fe5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load and process data\n",
    "\n",
    "# cmip model coordinates\n",
    "lat_model = nc.Dataset(model_file).variables['lat'][:].filled(np.nan)\n",
    "lon_model_raw = nc.Dataset(model_file).variables['lon'][:].filled(np.nan)\n",
    "z_model = (nc.Dataset(model_file).variables['depth'][:].filled(np.nan)) ## QDM processed file should have depth in m\n",
    "t_model = nc.Dataset(model_file).variables['time'][:].filled(np.nan)\n",
    "\n",
    "## Common grid has y, x in EPSG 3413 already\n",
    "x_model = nc.Dataset(model_file).variables['x'][:].filled(np.nan)\n",
    "y_model = nc.Dataset(model_file).variables['y'][:].filled(np.nan)\n",
    "\n",
    "# lon_model = np.mod((lon_model_raw+180), 360) -180 ## put back on -180 to 180 axis\n",
    "lon_model = lon_model_raw\n",
    "# lon_grid, lat_grid = np.meshgrid(lon_model, lat_model) ## necessary if 1D arrays\n",
    "\n",
    "\n",
    "# # get coordinates in EPSG:3413 for consistency with ismip grid\n",
    "# mapping = Transformer.from_crs(\"epsg:4326\", \"epsg:3413\", always_xy=True)\n",
    "# x_model, y_model = mapping.transform(lon_model, lat_model)\n",
    "\n",
    "# ## load TF\n",
    "## No masking, just set <0 to 0 and allow all other missing data -- should be missing where there is no ocean data in domain\n",
    "TF_model_raw = nc.Dataset(model_file).variables['TF'][:]\n",
    "TF_model_nonneg = np.ma.masked_less(TF_model_raw, 0) ## filter out erroneous negative values\n",
    "# TF_model_pos = np.ma.filled(TF_model_nonneg, fill_value=0) ## and assign 0\n",
    "# TF_model_nonneg[TF_model_raw<0] = 0\n",
    "# TF_model = np.ma.masked_greater(TF_model_nonneg, 1.1e10) ## filter out fill value\n",
    "TF_model = TF_model_raw\n",
    "TF_model[TF_model<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe99b5-043e-48b6-a0c5-81b5a341cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(TF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d117e-4d3f-490b-ae2e-2a79e47ad124",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(TF_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f30be-4600-4814-8e91-b133c3a0f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we can tell from the read-in above that TF_model from EN4 has dimensions (depth, lat, lon, time)\n",
    "## change to ## depth, time, lat, lon as the code is expecting\n",
    "\n",
    "## for CESM2-WACCM raw, it is (time, depth, lat, lon)\n",
    "## for CESM2-WACCM QDM-corrected, it is (depth, nlat, nlon, time)\n",
    "## mean-corrected is (time, depth, y, x) \n",
    "## EN4 is (time, depth, y, x)\n",
    "\n",
    "## pre-gridded output should already be cropped and on effective depth grid?\n",
    "## commenting all below here 19 Sept\n",
    "## ---\n",
    "# # crop CMIP output to area of interest around Greenland\n",
    "# xlims = [np.min(x), np.max(x)]\n",
    "# ylims = [np.min(y), np.max(y)]\n",
    "# x_model = x_model.flatten()\n",
    "# y_model = y_model.flatten()\n",
    "# inds = np.where((x_model > xlims[0]) & (x_model < xlims[1]) & (y_model > ylims[0]) & (y_model < ylims[1]))[0]\n",
    "# x_model = x_model[inds]\n",
    "# y_model = y_model[inds]\n",
    "\n",
    "## make a filled version of the masked array for interpolation and shuffling\n",
    "TF_mod_filled = np.ma.filled(TF_model, fill_value=np.nan) ## filter out fill value\n",
    "# TF_shuffled = np.transpose(TF_mod_filled, axes=[0, 3, 1, 2]) ## for QDM-corrected\n",
    "TF_shuffled = np.transpose(TF_mod_filled, axes=[1, 0, 2, 3]) ## shuffle to depth, time, lat, lon\n",
    "TF_nop_model_orig_crop = TF_shuffled.reshape(TF_shuffled.shape[0], ## depth\n",
    "                                          TF_shuffled.shape[1], ## time?\n",
    "                                          TF_shuffled.shape[2]*TF_shuffled.shape[3]) ## should be x*y\n",
    "TF_nop_model_orig_crop = np.ma.masked_greater(TF_nop_model_orig_crop, 1.1e10)\n",
    "\n",
    "# # interpolate the thermal forcing from cmip vertical grid onto the effective depth grid\n",
    "# interp_TF = interp1d(z_model, TF_nop_model_orig_crop, axis=0, bounds_error=False, fill_value='extrapolate')\n",
    "# ## note which axis is depth/z. Donald's default had axis=1, but checking the shape of TF_model \n",
    "# ## showed that it had depth as its zeroth axis. Consistent with TF_shuffled order above\n",
    "\n",
    "# ## make z increase downwards -- consistent with CESM2-WACCM convention\n",
    "# z_inc_down = -1*z\n",
    "# TF_nop_model = interp_TF(z_inc_down)\n",
    "## ---\n",
    "\n",
    "xm, ym = np.meshgrid(x_model, y_model)\n",
    "x_mod = xm.flatten()\n",
    "y_mod = ym.flatten()\n",
    "\n",
    "# get max depth at which each cmip point has a thermal forcing value\n",
    "depth_model = np.full(x_mod.shape, np.nan)\n",
    "for i in range(len(x_mod)):\n",
    "    tf = TF_nop_model_orig_crop[:,0,i] ## careful with order of axes here\n",
    "    if np.all(np.isnan(tf)):\n",
    "        continue\n",
    "    elif np.all(~np.isnan(tf)):\n",
    "        depth_model[i] = z[-1]\n",
    "    else:\n",
    "        first_nan = np.argmax(np.isnan(tf))\n",
    "        depth_model[i] = z[first_nan-1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e35339e-2f38-4ab7-a41a-dbea108cc27e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TF_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e264cc9d-e68c-451a-ba8f-d394cbbde233",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TF_nop_model_orig_crop[0,0,:]) ## this should be equal to len(x_mod). If not, probably a mistake in shuffling axes above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfe09b-7582-4128-9dc9-eedaa8d0b0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8731fe13-ea40-44b9-98f1-bd73a08465d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plots to check things make sense\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# ax.contourf(lon_model, lat_model, TF_model[0,0, :,:]) ## for raw\n",
    "# ax.contourf(lon_model, lat_model, TF_model[0,:,:,0]) ## for QDM-processed\n",
    "ax.contourf(lon_model, lat_model, TF_model[0,0,:,:]) ## for mean-corrected\n",
    "plt.show()\n",
    "\n",
    "# plt.scatter(xm, ym, c=TF_nop_model[0,0,:])\n",
    "# plt.colorbar(label='thermal forcing at first time and depth slice (°C)')\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "plt.scatter(x_mod, y_mod, c=depth_model)\n",
    "plt.colorbar(label='max depth of non-NaN data in CMIP model (m)')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfe540a-cd23-4dee-87d8-9e97759fd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_eff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031c388-797c-43af-8eeb-52bf85444be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c147b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## match ismip points to cmip points\n",
    "\n",
    "# linear versions of effective position arrays\n",
    "X_eff_flat = X_eff.flatten()\n",
    "Y_eff_flat = Y_eff.flatten()\n",
    "\n",
    "# initialise arrays that will store the linear index of the required cmip point\n",
    "x_ind = np.full_like(X_eff_flat, np.nan, dtype=float)\n",
    "z_ind = np.full_like(X_eff_flat, np.nan, dtype=float)\n",
    "\n",
    "# loop over effective depth grid\n",
    "for k, z_val in enumerate(z):\n",
    "    \n",
    "    # get all ismip points with this effective depth\n",
    "    i_inds = np.where(z_eff.flatten() == z_val)[0]\n",
    "\n",
    "    # save the effective depth index\n",
    "    z_ind[i_inds] = k\n",
    "\n",
    "    # get all cmip points that have data at this depth\n",
    "    c_inds = np.where(depth_model.flatten() <= z_val)[0]\n",
    "\n",
    "    # loop over ismip points with this effective depth and find closest cmip point to effective position\n",
    "    for i in i_inds:\n",
    "        dsq = (x_mod[c_inds] - X_eff_flat[i])**2 + (y_mod[c_inds] - Y_eff_flat[i])**2\n",
    "        id_min = np.argmin(dsq)\n",
    "        x_ind[i] = c_inds[id_min]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5420f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now we fill the ismip TF using the matched cmip positions and depths\n",
    "\n",
    "# initialise array\n",
    "TF_nop = np.full((len(t_model), len(x_ind)), np.nan)\n",
    "\n",
    "# fill array knowing that the ismip TF(t,i) equals the cmip TF(z_ind(i),t,x_ind(i))\n",
    "for i in range(len(x_ind)):\n",
    "    if not np.isnan(x_ind[i]):\n",
    "        ## depth first\n",
    "        TF_nop[:,i] = TF_nop_model_orig_crop[int(z_ind[i]), :, int(x_ind[i])]\n",
    "\n",
    "# reshape array for consistency with ismip coordinates\n",
    "TF_nop = TF_nop.reshape(len(t_model), X.shape[0], X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258dda7c-c790-4881-917d-95dde8d9fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## final corrections\n",
    "\n",
    "## Freezing point dependence on pressure already included Step 1 computation\n",
    "if PressureAlreadyIncluded:\n",
    "    TF = TF_nop\n",
    "else:\n",
    "    ## correct for the freezing point dependence on pressure\n",
    "    TF = np.full_like(TF_nop, np.nan, dtype=float)\n",
    "    l3 = 7.61e-4; # dependence of freezing point on depth\n",
    "    for k in range(TF_nop.shape[0]):\n",
    "        TF[k, :, :] = TF_nop[k, :, :] - l3 * z_eff\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4228bec0-599d-43c5-a4df-543fa0a62e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check if any TF are less than 0 and fix\n",
    "# TF = np.full_like(TF_nop, np.nan, dtype=float) ## no pressure correction\n",
    "inds = np.where(TF < 0)\n",
    "num_corrected = len(inds[0])\n",
    "if num_corrected > 0:\n",
    "    TF[inds] = 0\n",
    "    print(f\"Warning: {num_corrected} pixels corrected for TF<0\")\n",
    "\n",
    "# all the unconnected below sea level points must be assigned TF=0 -  these points have z_eff=NaN\n",
    "TF[:, np.isnan(z_eff)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdaa6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot an example of the result\n",
    "\n",
    "plt.pcolormesh(X, Y, TF[0, :, :])\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('extrapolated thermal forcing at first time slice (°C)')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d06e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## write to output file\n",
    "# from datetime import date\n",
    "\n",
    "# if PressureAlreadyIncluded:\n",
    "#     p_tag = 'PFromStep1'\n",
    "# else:\n",
    "#     p_tag = 'PCorrected_Step3'\n",
    "\n",
    "# # file name\n",
    "# output_file = 'TF-{}-ISMIP_Grid-{}-QDM_corrected-{}.nc'.format(SelModel, p_tag, date.today())\n",
    "\n",
    "# # delete previous file if it exists\n",
    "# if os.path.exists(output_file):\n",
    "#     os.remove(output_file)\n",
    "\n",
    "# # write netcdf\n",
    "# with Dataset(output_file, 'w', format='NETCDF4') as nc:\n",
    "\n",
    "#     # dimensions\n",
    "#     nc.createDimension('x', len(x))\n",
    "#     nc.createDimension('y', len(y))\n",
    "#     nc.createDimension('t', len(t_model))\n",
    "\n",
    "#     # variables\n",
    "#     x_var = nc.createVariable('x', np.float32, ('x',))  # 'f4' = single precision float\n",
    "#     y_var = nc.createVariable('y', np.float32, ('y',))\n",
    "#     t_var = nc.createVariable('t', np.float32, ('t',))\n",
    "#     TF_var = nc.createVariable('TF', np.float32, ('t', 'y', 'x'), zlib=True, complevel=9)\n",
    "\n",
    "#     # Write data\n",
    "#     x_var[:] = np.array(x, dtype=np.float32)\n",
    "#     y_var[:] = np.array(y, dtype=np.float32)\n",
    "#     t_var[:] = np.array(t_model, dtype=np.float32)\n",
    "#     TF_var[:, :, :] = np.array(TF, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94c049-fea3-4d32-ad06-d8047942a310",
   "metadata": {},
   "source": [
    "## Write out\n",
    "Use xarray to write nice metadata and keep date format intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30023079-26d8-40a5-856a-35eb49239f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_out_ds = xr.Dataset(\n",
    "    data_vars = dict(TF=(['time', 'y', 'x'], TF)), \n",
    "    coords = dict(\n",
    "        time = ds_model.time, ## cheat a bit and take the datetime64 index from ds_model, read in above\n",
    "        y = y,\n",
    "        x = x)\n",
    ")\n",
    "\n",
    "TF_out_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d109c4-6685-4492-84a7-b73d66df17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_out_ds = TF_out_ds.convert_calendar('noleap')\n",
    "TF_out_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31f7ca1-8d0b-43b4-807d-be126c83b2f7",
   "metadata": {},
   "source": [
    "Explicitly set some metadata, and specify that TF should be written as float32 (like Donald did in his approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555a2e42-8b8d-4eed-bbdb-3d5b568ca5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_out_ds.coords['y'].attrs['units'] = 'meters'\n",
    "TF_out_ds.coords['y'].attrs['projection'] = 'EPSG:3413'\n",
    "TF_out_ds.coords['x'].attrs['units'] = 'meters'\n",
    "TF_out_ds.coords['x'].attrs['projection'] = 'EPSG:3413'\n",
    "\n",
    "TF_out_ds.TF.attrs['long_name'] = 'Ocean thermal forcing'\n",
    "TF_out_ds.TF.attrs['fill_value'] = 1.1e20 ## list fillvalue even though we have not done explicit filling in this step\n",
    "TF_out_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28729837-f8cb-4066-a459-967be683dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_reduced = TF_out_ds.astype('float32')\n",
    "TF_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84189705-42ad-472c-a597-b1dc884a1abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into two datasets for writing\n",
    "\n",
    "TF_reduced_hist = TF_reduced.sel(time=slice('1850','2014'))\n",
    "# TF_reduced_hist\n",
    "TF_reduced_fut = TF_reduced.sel(time=slice('2015','2100'))\n",
    "TF_reduced_fut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577490c-b641-455c-81df-0c428693ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SINGLE OUTPUT FILE\n",
    "# ## write to output file\n",
    "# from datetime import datetime\n",
    "\n",
    "# now = datetime.now()\n",
    "\n",
    "# ## Make filename tags showing time for the output \n",
    "# ## this is very janky, but datetime64 objects are stubborn\n",
    "# FirstYear = TF_reduced.time[0].values.astype(str).split('-')[0] \n",
    "# LastYear = TF_reduced.time[-1].values.astype(str).split('-')[0]\n",
    "\n",
    "# # FirstYear=2015\n",
    "# # LastYear=2100\n",
    "# # ##REMEMBER TO CHANGE BACK!  This one only for CESM raw\n",
    "\n",
    "# if PressureAlreadyIncluded:\n",
    "#     p_tag = 'PFromStep1'\n",
    "# else:\n",
    "#     p_tag = 'PCorrected_Step3'\n",
    "\n",
    "# SelModel='CESM2_WACCM_raw'\n",
    "\n",
    "# ## file name\n",
    "# out_fn = DirSave + 'TF-ISMIP_Grid-{}-{}_{}-{}-{}.nc'.format(SelModel, \n",
    "#                                                     FirstYear, LastYear, \n",
    "#                                                     p_tag, \n",
    "#                                                     now.strftime('%Y%m%d'))\n",
    "\n",
    "# # ds_temp = tf_out.to_dataset(name='TF')\n",
    "# ds_out = TF_reduced.assign_attrs(title='Ocean thermal forcing for {}'.format(SelModel),\n",
    "#                              summary='TF computed following Verjans bias correction and Slater inland mapping,' + \n",
    "#                              'in a bounding box around Greenland, for ISMIP7 Greenland forcing.' +\n",
    "#                                 # ' Mean correction based on EN4, 1985-2014.' +\n",
    "#                                 ' Process code: github.com/ehultee/gris-iceocean-process',\n",
    "#                              institution='NASA Goddard Space Flight Center',\n",
    "#                              creation_date=now.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# ## write it!\n",
    "# ds_out.to_netcdf(path=out_fn,\n",
    "#                 encoding={'TF': {'zlib': True, 'complevel':9}}) ## set compression level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7df1ef-9bc5-40d0-95ea-597522a00db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MULTI OUTPUT FILE\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "to_write = [TF_reduced_hist,TF_reduced_fut]\n",
    "\n",
    "for d in to_write:\n",
    "    print('Processing...')\n",
    "    ## Make filename tags showing time for the output \n",
    "    # ## this is very janky, but datetime64 objects are stubborn\n",
    "    # FirstYear = d.time[0].values.astype(str).split('-')[0] \n",
    "    # LastYear = d.time[-1].values.astype(str).split('-')[0]\n",
    "    ## with cftime axes:\n",
    "    FirstYear = d.time[0].values.item().year\n",
    "    LastYear = d.time[-1].values.item().year\n",
    "    \n",
    "    # FirstYear=2015\n",
    "    # LastYear=2100\n",
    "    # ##REMEMBER TO CHANGE BACK!  This one only for CESM raw\n",
    "    \n",
    "    if PressureAlreadyIncluded:\n",
    "        p_tag = 'PFromStep1'\n",
    "    else:\n",
    "        p_tag = 'PCorrected_Step3'\n",
    "    \n",
    "    SelModel='CESM2-WACCM'\n",
    "    \n",
    "    ## file name\n",
    "    out_fn = DirSave + 'TF_MeanCorrected-ISMIP_Grid-{}-{}_{}-{}-{}.nc'.format(SelModel, \n",
    "                                                        FirstYear, LastYear, \n",
    "                                                        p_tag, \n",
    "                                                        now.strftime('%Y%m%d'))\n",
    "    print('Assigning attrs')\n",
    "    # ds_temp = tf_out.to_dataset(name='TF')\n",
    "    ds_out = d.assign_attrs(title='Ocean thermal forcing for {}'.format(SelModel),\n",
    "                                 summary='TF computed following Verjans bias correction and Slater inland mapping,' + \n",
    "                                 'in a bounding box around Greenland, for ISMIP7 Greenland forcing.' +\n",
    "                                    ' Mean correction based on EN4, 1985-2014.' +\n",
    "                                    ' Process code: github.com/ehultee/gris-iceocean-process',\n",
    "                                 institution='NASA Goddard Space Flight Center',\n",
    "                                 creation_date=now.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    print('Writing')\n",
    "    ## write it!\n",
    "    # with ProgressBar():\n",
    "    ds_out.to_netcdf(path=out_fn,\n",
    "                    encoding={'TF': {'zlib': True, 'complevel':9}}) ## set compression level\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea81564-e01e-43fe-9b9c-241d3a13a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_reduced_hist.time[0].values.astype(str).item().split('-')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b270cd-bcdd-4264-8e54-560419a5ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_reduced_hist.time[0].values.item().year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6129ce58-b39c-4d7f-9ecf-55f9b9873038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
