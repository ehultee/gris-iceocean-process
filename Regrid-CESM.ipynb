{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236564fb-fc82-49c5-93f2-8630422b55d6",
   "metadata": {},
   "source": [
    "## Regrid CESM2-WACCM TF\n",
    "CESM2-WACCM is in a rotated polar grid with dimensions `nlat, nlon` rather than `lat, lon`.  Use rioxarray to regrid this to match EN4 grid before applying QDM bias correction.\n",
    "\n",
    "Note: depth/`lev` variable may be in cm rather than m?\n",
    "\n",
    "7 Jul 2025 | EHU\n",
    "- Test with newly processed TF data, which accounts for depth expressed in cm rather than m in the original CESM2-WACCM data.  Note that the depth variable in the TF dataset is most likely still in cm.\n",
    "- 11 Jul: Project directly to EN4 projection, rather than a rectilinear grid based on the CESM curvilinear grid in between\n",
    "- 15 Jul: Fix `warp.reproject` command to get correct length of time dimension for CESM. Test `reproject_match` command -- still seems necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aca593-f9c2-4754-ab2b-04487467d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import cartopy.crs as ccrs ## map projections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rioxarray\n",
    "from rioxarray.rioxarray import affine_to_coords\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a7c19-075e-432b-9461-36c98031d1c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## from cmethods.utils\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Optional, Union, TypeVar\n",
    "\n",
    "XRData_t = (xr.Dataset, xr.DataArray)\n",
    "NPData_t = (list, np.ndarray, np.generic)\n",
    "XRData = TypeVar(\"XRData\", xr.Dataset, xr.DataArray)\n",
    "NPData = TypeVar(\"NPData\", list, np.ndarray, np.generic)\n",
    "MAX_SCALING_FACTOR = 2 ## to allow multiplicative correction?\n",
    "\n",
    "\n",
    "def check_xr_types(obs: XRData, simh: XRData, simp: XRData) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type xarray.core.dataarray.Dataset or xarray.core.dataarray.DataArray\"\n",
    "\n",
    "    if not isinstance(obs, XRData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, XRData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, XRData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def check_np_types(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type list, np.ndarray or np.generic\"\n",
    "\n",
    "    if not isinstance(obs, NPData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, NPData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, NPData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def nan_or_equal(value1: float, value2: float) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the values are equal or at least one is NaN\n",
    "\n",
    "    :param value1: First value to check\n",
    "    :type value1: float\n",
    "    :param value2: Second value to check\n",
    "    :type value2: float\n",
    "    :return: If any value is NaN or values are equal\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    return np.isnan(value1) or np.isnan(value2) or value1 == value2\n",
    "        \n",
    "def ensure_dividable(\n",
    "    numerator: Union[float, np.ndarray],\n",
    "    denominator: Union[float, np.ndarray],\n",
    "    max_scaling_factor: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensures that the arrays can be divided. The numerator will be multiplied by\n",
    "    the maximum scaling factor of the CMethods class if division by zero.\n",
    "\n",
    "    :param numerator: Numerator to use\n",
    "    :type numerator: np.ndarray\n",
    "    :param denominator: Denominator that can be zero\n",
    "    :type denominator: np.ndarray\n",
    "    :return: Zero-ensured division\n",
    "    :rtype: np.ndarray | float\n",
    "    \"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        result = numerator / denominator\n",
    "\n",
    "    if isinstance(numerator, np.ndarray):\n",
    "        mask_inf = np.isinf(result)\n",
    "        result[mask_inf] = numerator[mask_inf] * max_scaling_factor  # type: ignore[index]\n",
    "\n",
    "        mask_nan = np.isnan(result)\n",
    "        result[mask_nan] = 0  # type: ignore[index]\n",
    "    elif np.isinf(result):\n",
    "        result = numerator * max_scaling_factor\n",
    "    elif np.isnan(result):\n",
    "        result = 0.0\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_pdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Compuites and returns the the probability density function :math:`P(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: The vector to get :math:`P(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`P(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The probability densitiy function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the probability density function :math:`P(x)`\n",
    "\n",
    "        >>> from cmethods get_pdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_pdf(x=x, xbins=xbins))\n",
    "        [2, 5, 5]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def get_cdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Computes and returns returns the cumulative distribution function :math:`F(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: Vector to get :math:`F(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`F(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The cumulative distribution function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the cumulative distribution function :math:`F(x)`\n",
    "\n",
    "        >>> from cmethods.utils import get_cdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_cdf(x=x, xbins=xbins))\n",
    "        [0.0, 0.16666667, 0.58333333, 1.]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    cdf = np.insert(np.cumsum(pdf), 0, 0.0)\n",
    "    return cdf / cdf[-1]\n",
    "\n",
    "\n",
    "def get_inverse_of_cdf(\n",
    "    base_cdf: Union[list, np.ndarray],\n",
    "    insert_cdf: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Returns the inverse cumulative distribution function as:\n",
    "    :math:`F^{-1}_{x}\\left[y\\right]` where :math:`x` represents ``base_cdf`` and\n",
    "    ``insert_cdf`` is represented by :math:`y`.\n",
    "\n",
    "    :param base_cdf: The basis\n",
    "    :type base_cdf: list | np.ndarray\n",
    "    :param insert_cdf: The CDF that gets inserted\n",
    "    :type insert_cdf: list | np.ndarray\n",
    "    :param xbins: Probability boundaries\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The inverse CDF\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    return np.interp(insert_cdf, base_cdf, xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c71f0-69e5-45ad-8663-62c9235338d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def quantile_delta_mapping(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    "    n_quantiles: int,\n",
    "    kind: str = \"+\",\n",
    "    **kwargs,\n",
    "    ) -> NPData:\n",
    "    r\"\"\"\n",
    "    Based on https://python-cmethods.readthedocs.io/en/latest/methods.html#quantile-delta-mapping\n",
    "\n",
    "    kind: str, default + for additive, can be set to * for multiplicative\n",
    "    \"\"\"\n",
    "    # check_adjust_called(\n",
    "    #     function_name=\"quantile_delta_mapping\",\n",
    "    #     adjust_called=kwargs.get(\"adjust_called\"),\n",
    "    # )\n",
    "    check_np_types(obs=obs, simh=simh, simp=simp)\n",
    "\n",
    "    if not isinstance(n_quantiles, int):\n",
    "        raise TypeError(\"'n_quantiles' must be type int\")\n",
    "\n",
    "    if kind=='+':\n",
    "        obs, simh, simp = (\n",
    "            np.array(obs),\n",
    "            np.array(simh),\n",
    "            np.array(simp),\n",
    "        )  # to achieve higher accuracy\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", min(np.nanmin(obs), np.nanmin(simh)))\n",
    "\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = abs(global_max - global_min) / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        # calculate exact CDF values of $F_{sim,p}[T_{sim,p}(t)]$\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "        delta = simp - get_inverse_of_cdf(cdf_simh, epsilon, xbins)  # Eq. 1.3\n",
    "        return QDM1 + delta  # Eq. 1.4\n",
    "\n",
    "    if kind=='*':\n",
    "        obs, simh, simp = np.array(obs), np.array(simh), np.array(simp)\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", 0.0)\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = global_max / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "\n",
    "        delta = ensure_dividable(  # Eq. 2.3\n",
    "            simp,\n",
    "            get_inverse_of_cdf(cdf_simh, epsilon, xbins),\n",
    "            max_scaling_factor=kwargs.get(\n",
    "                \"max_scaling_scaling\",\n",
    "                MAX_SCALING_FACTOR,\n",
    "            ),\n",
    "        )\n",
    "        return QDM1 * delta  # Eq. 2.4\n",
    "    raise NotImplementedError(\n",
    "        f\"{kind=} not available for quantile_delta_mapping. Use '+' or '*' instead.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_cmfunc(\n",
    "    method: str,\n",
    "    obs: XRData,\n",
    "    simh: XRData,\n",
    "    simp: XRData,\n",
    "    **kwargs: dict,\n",
    ") -> XRData:\n",
    "    \"\"\"\n",
    "    Internal function used to apply the bias correction technique to the\n",
    "    passed input data.\n",
    "    \"\"\"\n",
    "    ## hard-code the QDM method\n",
    "    if method!='quantile_delta_mapping':\n",
    "        raise UnknownMethodError('Not implemented for methods other than quantile_delta_mapping')\n",
    "        ## give this a default for what we want to do\n",
    "    else:\n",
    "        method='quantile_delta_mapping' ## not actually going to use this\n",
    "    \n",
    "    check_xr_types(obs=obs, simh=simh, simp=simp)\n",
    "    # if method not in __METHODS_FUNC__:\n",
    "    #     raise UnknownMethodError(method, __METHODS_FUNC__.keys())\n",
    "\n",
    "    if kwargs.get(\"input_core_dims\"):\n",
    "        if not isinstance(kwargs[\"input_core_dims\"], dict):\n",
    "            raise TypeError(\"input_core_dims must be an object of type 'dict'\")\n",
    "        if not len(kwargs[\"input_core_dims\"]) == 3 or any(\n",
    "            not isinstance(value, str) for value in kwargs[\"input_core_dims\"].values()\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                'input_core_dims must have three key-value pairs like: {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}',\n",
    "            )\n",
    "\n",
    "        input_core_dims = kwargs.pop(\"input_core_dims\")\n",
    "    else:\n",
    "        input_core_dims = {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}\n",
    "\n",
    "    result: XRData = xr.apply_ufunc(\n",
    "        quantile_delta_mapping,\n",
    "        obs,\n",
    "        simh,\n",
    "        # Need to spoof a fake time axis since 'time' coord on full dataset is\n",
    "        # different than 'time' coord on training dataset.\n",
    "        simp.rename({input_core_dims[\"simp\"]: \"__t_simp__\"}),\n",
    "        dask=\"parallelized\",\n",
    "        vectorize=True,\n",
    "        # This will vectorize over the time dimension, so will submit each grid\n",
    "        # cell independently\n",
    "        input_core_dims=[\n",
    "            [input_core_dims[\"obs\"]],\n",
    "            [input_core_dims[\"simh\"]],\n",
    "            [\"__t_simp__\"],\n",
    "        ],\n",
    "        # Need to denote that the final output dataset will be labeled with the\n",
    "        # spoofed time coordinate\n",
    "        output_core_dims=[[\"__t_simp__\"]],\n",
    "        kwargs=dict(kwargs),\n",
    "    )\n",
    "\n",
    "    # Rename to proper coordinate name.\n",
    "    result = result.rename({\"__t_simp__\": input_core_dims[\"simp\"]})\n",
    "\n",
    "    # ufunc will put the core dimension to the end (time), so want to preserve\n",
    "    # original order where time is commonly first.\n",
    "    return result.transpose(*obs.rename({input_core_dims[\"obs\"]: input_core_dims[\"simp\"]}).dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fae5d-a8d9-4c62-a6f8-39926ff4bda3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Time utils from Bryan Riel\n",
    "## pasting stuff from iceutils below.\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "def tdec2datestr(tdec_in, returndate=False):\n",
    "    \"\"\"\n",
    "    Convert a decimaly year to an iso date string.\n",
    "    \"\"\"\n",
    "    if isinstance(tdec_in, (list, np.ndarray)):\n",
    "        tdec_list = copy.deepcopy(tdec_in)\n",
    "    else:\n",
    "        tdec_list = [tdec_in]\n",
    "    current_list = []\n",
    "    for tdec in tdec_list:\n",
    "        year = int(tdec)\n",
    "        yearStart = datetime.datetime(year, 1, 1)\n",
    "        if year % 4 == 0:\n",
    "            ndays_in_year = 366.0\n",
    "        else:\n",
    "            ndays_in_year = 365.0\n",
    "        days = (tdec - year) * ndays_in_year\n",
    "        seconds = (days - int(days)) * 86400\n",
    "        tdelta = datetime.timedelta(days=int(days), seconds=int(seconds))\n",
    "        current = yearStart + tdelta\n",
    "        if not returndate:\n",
    "            current = current.isoformat(' ').split()[0]\n",
    "        current_list.append(current)\n",
    "\n",
    "    if len(current_list) == 1:\n",
    "        return current_list[0]\n",
    "    else:\n",
    "        return np.array(current_list)\n",
    "\n",
    "\n",
    "def datestr2tdec(yy=0, mm=0, dd=0, hour=0, minute=0, sec=0, microsec=0, dateobj=None):\n",
    "    \"\"\"\n",
    "    Convert year, month, day, hours, minutes, seconds to decimal year.\n",
    "    \"\"\"\n",
    "    if dateobj is not None:\n",
    "        if type(dateobj) == str:\n",
    "            yy, mm, dd = [int(val) for val in dateobj.split('-')]\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        elif type(dateobj) == datetime.datetime:\n",
    "            attrs = ['year', 'month', 'day', 'hour', 'minute', 'second']\n",
    "            yy, mm, dd, hour, minute, sec = [getattr(dateobj, attr) for attr in attrs]\n",
    "        elif type(dateobj) == np.datetime64:\n",
    "            yy = dateobj.astype('datetime64[Y]').astype(int) + 1970\n",
    "            mm = dateobj.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "            days = (\n",
    "                (dateobj - dateobj.astype('datetime64[M]')) / np.timedelta64(1, 'D')\n",
    "            )\n",
    "            dd = int(days) + 1\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        else:\n",
    "            raise NotImplementedError('dateobj must be str, datetime, or np.datetime64.')\n",
    "\n",
    "    # Make datetime object for start of year\n",
    "    yearStart = datetime.datetime(yy, 1, 1, 0, 0, 0)\n",
    "    # Make datetime object for input time\n",
    "    current = datetime.datetime(yy, mm, dd, hour, minute, sec, microsec)\n",
    "    # Compute number of days elapsed since start of year\n",
    "    tdelta = current - yearStart\n",
    "    # Convert to decimal year and account for leap year\n",
    "    if yy % 4 == 0:\n",
    "        return float(yy) + tdelta.total_seconds() / (366.0 * 86400)\n",
    "    else:\n",
    "        return float(yy) + tdelta.total_seconds() / (365.0 * 86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3e1d9-d32c-43d4-98fe-453b57cf72a4",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2b62c-dc36-4101-b755-09d257c01176",
   "metadata": {},
   "outputs": [],
   "source": [
    "DepthRange         = [0,500]\n",
    "ShallowThreshold   = 100\n",
    "PeriodObs0         = [1950,2015]\n",
    "SelModel = 'CESM'\n",
    "\n",
    "DirSave = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/Summer25Test'\n",
    "DirIn = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/Summer25Test'\n",
    "\n",
    "DirHadley = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles'\n",
    "HadleyFile = f'/tf-Hadley-1950_2020.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1016d0-9be8-4745-bfcf-06b0c09b86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load EN4 using xarray\n",
    "ds1 = xr.open_dataset(DirHadley+HadleyFile, decode_times='timeDim')\n",
    "ds1\n",
    "# ds2 = ds.assign_coords({'timeDim': ds.time, \n",
    "#                   'latDim': ds.lat, \n",
    "#                   'lonDim': ds.lon,\n",
    "#                   'depthDim': ds.depth})\n",
    "\n",
    "# tfEN4 = ds2.tfdpavg0to500_bathymin100.rename({'timeDim': 'time',\n",
    "#                                               'latDim': 'lat',\n",
    "#                                               'lonDim': 'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76909c05-e896-4ead-a285-03f0f42e7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in CESM TF for all time slices available, using multifile dataset\n",
    "with xr.open_mfdataset(f'{DirIn}/tf-CESM2*.nc') as ds: \n",
    "    ds3 = ds.load()\n",
    "\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023e615-b000-4f38-bac1-5df8f06f1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_m = ds3.where(ds3.TF<1e20)\n",
    "ds_m.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fb8d8-20eb-462e-a0ad-852447dbe9da",
   "metadata": {},
   "source": [
    "Depth resampling seemed to be smearing fill value across depth levels, so that the mean of depth-resampled dataset was ~5e17, even when fill values masked out.  Created `ds_m`  before resampling to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a2339-4bb6-479b-9583-7434c0b11de6",
   "metadata": {},
   "source": [
    "## Express depth in m, then resample to Hadley depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298aff7-1fa0-4bf9-927d-7974b9c709a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## resample CESM to Hadley depths?\n",
    "# ds1.depth\n",
    "tf_CESM_inM = ds_m.assign_coords(new_depth = ('lev', ds3.indexes['lev'].values*0.01))\n",
    "tf_CESM_inM = tf_CESM_inM.drop_indexes('lev')\n",
    "tf_CESM_inM = tf_CESM_inM.set_xindex('new_depth').drop_vars('lev')\n",
    "tf_CESM_inM = tf_CESM_inM.rename({'new_depth': 'lev'})\n",
    "tf_CESM_inM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64ea12-1605-4f9c-bb86-3ff4364b7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfCESM_resampled = tf_CESM_inM.interp(lev=ds1.depth.values[0:30]).rename({'lev': 'depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8817c1-9f97-436e-a044-28ba5b4af554",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfCESM_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25293c48-28b4-4def-a4ca-85348b366bc6",
   "metadata": {},
   "source": [
    "We had to do one extra step (above) to deal with re-scaling the native depth dimension from cm to m. Then we re-sampled to depth levels that match EN4. Now we proceed with applying a DateTimeIndex, and reprojecting from the rotated pole spatial grid to a regular grid matching EN4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4553ab-0a8e-469a-bca6-7fcec4084128",
   "metadata": {},
   "source": [
    "### Apply DateTimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180b978-d7ed-4119-b9f5-2717422b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_full = tfCESM_resampled.TF.sel( \n",
    "                    time=slice('1950', '2020'))\n",
    "\n",
    "## aligning the time indices\n",
    "test_ds_full = test_ds_full.assign_coords(new_time = ('time', test_ds_full.indexes['time'].to_datetimeindex().values))\n",
    "test_ds_full = test_ds_full.drop_indexes('time')\n",
    "test_ds_full = test_ds_full.set_xindex('new_time').drop_vars('time')\n",
    "\n",
    "## aligning the names of the variables between obs and sim\n",
    "test_ds_full = test_ds_full.to_dataset()\n",
    "test_ds_full = test_ds_full.rename({'new_time': 'time'})\n",
    "test_ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54f475-c63a-491e-9a70-a886609afd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tobs_ds_full = ds1.TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee14242-e3b1-40ac-80b1-8a2ba4819470",
   "metadata": {},
   "source": [
    "### Reproject obs to match CESM grid\n",
    "Because the grids are offset from each other, we will need to warp/resample using `rioxarray` before we run the QDM correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d1244-4a1e-4ac7-8991-5aee6d93b170",
   "metadata": {},
   "source": [
    "Note that the rotated-pole grid of CESM2-WACCM is different from the original test case.  Unlikely we can use reproject_match in this case.  Try [something else](https://gist.github.com/j08lue/e792b3c912c33e9191734af7e795b75c) with rasterio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19352004-bb84-4cae-b22f-a45a83335142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f9f4e-f6f5-4e8a-9695-de3651e7db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm that this version has src_geoloc_array option -- must be 1.4 or greater\n",
    "rasterio.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b735f-5ff5-405c-82ec-667027c109f5",
   "metadata": {},
   "source": [
    "11 Jul: try going directly to the EN4 projection, instead of a default rectilinear and then from there to EN4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84a51c-b68b-4f79-903a-2fb8aa60b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_ds_full.sel(depth=500, method='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a6681f-be38-4a55-9ab1-a5d436842851",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon2d = test_ds_full[\"lon\"].values\n",
    "lat2d = test_ds_full[\"lat\"].values\n",
    "src_height, src_width = lon2d.shape\n",
    "\n",
    "WGS84 = rasterio.crs.CRS.from_epsg(4326)\n",
    "\n",
    "# dst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n",
    "#     src_crs=WGS84,\n",
    "#     dst_crs=WGS84,\n",
    "#     width=src_width,\n",
    "#     height=src_height,\n",
    "#     src_geoloc_array=(lon2d, lat2d),\n",
    "# )\n",
    "\n",
    "lon_EN4 = tobs_ds_full['lon'].values\n",
    "lat_EN4 = tobs_ds_full['lat'].values\n",
    "\n",
    "dst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n",
    "    src_crs=WGS84,\n",
    "    dst_crs=WGS84,\n",
    "    width=len(lon_EN4),\n",
    "    height=len(lat_EN4),\n",
    "    src_geoloc_array=(lon2d, lat2d),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5876f0-6a80-4a48-84cb-f80d34cc0b69",
   "metadata": {},
   "source": [
    "11 Jul: This reproj worked with no errors, but later cells show that of course this is going to cause problems, because we have time slices and so `len` shows the length of the time dimension rather than the data values at a given time step. \n",
    "\n",
    "15 Jul: Fixed problem with time dimension -- take the length of `destination` from the CESM time dimension `test_ds_full.time` rather than from EN4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28771018-54d2-4dcf-a6dd-42c49b8235c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_ds_full.sel(depth=500, method='nearest').time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a2210-e7c4-460e-9830-06c75323f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "## make rectilinear\n",
    "source = test_ds_full['TF'].sel(depth=500, method='nearest').values\n",
    "# destination = np.full((len(source), dst_height, dst_width), np.nan)\n",
    "## go straight to EN4\n",
    "## first value in np.full((t,y,x)) is length of the time dimension\n",
    "destination = np.full((len(test_ds_full.sel(depth=500, method='nearest').time),\n",
    "                          dst_height, dst_width), np.nan)\n",
    "\n",
    "data, transform = rasterio.warp.reproject(\n",
    "    source,\n",
    "    destination=destination,\n",
    "    src_crs=WGS84,\n",
    "    dst_crs=WGS84,\n",
    "    dst_transform=dst_transform,\n",
    "    dst_nodata=np.nan, ## previously had the Verjans fill value here, but now that we've done\n",
    "    ## a `where` command to mask the dataset before depth resampling, the missing\n",
    "    ## values have been replaced by NaNs\n",
    "    src_geoloc_array=np.stack((lon2d, lat2d))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4fa1de-3fd9-4eee-ba93-44fb666d2045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = np.ma.masked_where(source>1e+20, source)\n",
    "np.nanmean(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d93a14-040f-477a-b381-1b4700a3f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.nanmean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310845f8-eee4-4fee-ab3f-fd739dbd0e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = affine_to_coords(transform, width=dst_width, height=dst_height, x_dim=\"x\", y_dim=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be53e7-a50e-4035-9677-37c47a2b3661",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords.update(time=test_ds_full[\"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f15817b-7bbe-47f3-adea-eee2b70a8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_attrs = test_ds_full['TF'].attrs.copy()\n",
    "filtered_attrs.pop(\"grid_mapping\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d21c0f-c563-41f1-9b06-f53aa827520a",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = xr.DataArray(data, coords=coords, dims=(\"time\", \"y\", \"x\"), name='TF', attrs=filtered_attrs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b38a80e-6c3b-4b5e-9809-694ae1124856",
   "metadata": {},
   "source": [
    "7 Jul: first tested the naive reprojection, writing the CRS with x_dim=\"lon\" and y_dim=\"lat\" even though the dims are nlon, nlat (two dimensional, rotated pole system).  This didn't work.  Try with the rectilinear array.\n",
    "\n",
    "Note that it looks like we'll have to do this process at each level as a DataArray rather than on the whole Dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a396d1-7d94-40af-8621-eadf91b6ea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_masked = da.where(da<1.0e20) ## remove fill values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "ax = plt.axes(projection=ccrs.Robinson())\n",
    "# ax.set_xlim(da_masked.x.max(), da_masked.x.min())\n",
    "# ax.set_ylim(da_masked.y.min(), da_masked.y.max())\n",
    "da_masked.sel(time='1990-01-01', method='nearest').plot(ax=ax, transform=ccrs.PlateCarree()) ## specify x and y coordinates\n",
    "ax.set_extent([da_masked.x.min(), da_masked.x.max(),\n",
    "               da_masked.y.min(), da_masked.y.max()], crs=ccrs.PlateCarree())\n",
    "ax.coastlines(); ax.gridlines();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be43616-a1a7-4e2f-9d52-bec4bd1a2147",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_masked.x.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30e040-e511-47b4-8878-e67dcee77a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_masked.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838312ff-630c-4b02-af47-06bcce94e884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc5848c5-3354-4947-9a69-a697798f7e6f",
   "metadata": {},
   "source": [
    "## Assign spatial reference\n",
    "It seems that this `warp.reproject` sequence above has not brought the CESM test set (`da_masked`) into the same grid as the EN4 observations.  `tobs_ds_full` has very different spatial dimensions, as we can see below. Maybe we still need to do a `reproject_match` command with the spatial reference specified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3950af2-3930-4670-aaa7-2534f579095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73ccaec-b992-4529-8211-3295ab0215f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "da_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8f0881-8de8-41ea-82cb-58795669146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CRS(\"EPSG:4326\")\n",
    "\n",
    "tobs_ds_full.rio.write_crs(cc, inplace=True).rio.set_spatial_dims(\n",
    "    x_dim=\"lon\",\n",
    "    y_dim=\"lat\",\n",
    "    inplace=True,\n",
    ").rio.write_coordinate_system(inplace=True)\n",
    "\n",
    "da_masked.rio.write_crs(cc, inplace=True).rio.set_spatial_dims(\n",
    "    x_dim=\"x\",\n",
    "    y_dim=\"y\",\n",
    "    inplace=True,\n",
    ").rio.write_coordinate_system(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9cb98a-b545-431c-88d6-55e15c381ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test reproject_match\n",
    "test_d = 500\n",
    "\n",
    "# sim_level = da_masked.sel(depth=test_d, method='nearest')\n",
    "sim_level = da_masked ## already a depth slice\n",
    "obs_level = tobs_ds_full.sel(depth=test_d, method='nearest')\n",
    "obs_repr = obs_level.rio.reproject_match(sim_level)\n",
    "# obs_slices[d] = obs_repr\n",
    "# # obs_slices[d] = {'dims': (\"time\", \"lat\", \"lon\"), 'data': obs_repr.data}\n",
    "# sim_slices[d] = sim_level\n",
    "\n",
    "# obtemp = xr.concat([obs_slices[d] for d in tobs_ds_full.depth.values[0:30]], dim=tobs_ds_full.depth.values[0:30])\n",
    "# obtemp = obtemp.drop_vars('depth') ## drop a 1D depth variable that carried through\n",
    "# simtemp = xr.concat([sim_slices[d] for d in tobs_ds_full.depth.values[0:30]], dim=tobs_ds_full.depth.values[0:30])\n",
    "# simtemp = simtemp.drop_vars('depth')\n",
    "\n",
    "# tobs_repr_match = obtemp.rename({'concat_dim': 'depth', 'x': 'lon', 'y': 'lat'})\n",
    "# tsim_match = simtemp.rename({'concat_dim': 'depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d92bc36-381e-40f0-9035-85bf13def247",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a6c151-a0df-47ec-a459-be44b921e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19b714-e99e-4c76-9e9b-3b7f4711c978",
   "metadata": {},
   "source": [
    "Okay, this will work fine on slices.  Clean up the process now so that we apply consistent spatial reference, perform slicing of the dataset only once, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9342438b-f84a-47d9-8d09-d17267481991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
