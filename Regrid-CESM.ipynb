{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236564fb-fc82-49c5-93f2-8630422b55d6",
   "metadata": {},
   "source": [
    "## Regrid CESM2-WACCM TF\n",
    "CESM2-WACCM is in a rotated polar grid with dimensions `nlat, nlon` rather than `lat, lon`.  Use rioxarray to regrid this to match EN4 grid before applying QDM bias correction.\n",
    "\n",
    "Note: depth/`lev` variable may be in cm rather than m?\n",
    "\n",
    "7 Jul 2025 | EHU\n",
    "- Test with newly processed TF data, which accounts for depth expressed in cm rather than m in the original CESM2-WACCM data.  Note that the depth variable in the TF dataset is most likely still in cm.\n",
    "- 11 Jul: Project directly to EN4 projection, rather than a rectilinear grid based on the CESM curvilinear grid in between\n",
    "- 15 Jul: Fix `warp.reproject` command to get correct length of time dimension for CESM. Test `reproject_match` command -- still seems necessary.  Process all depth slices using `warp.reproject` and `reproject_match` together.  TODO: consider writing out an intermediate file, or re-implementing with Dask. Process is fairly slow and tends to kill the Jupyter kernel when processing locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aca593-f9c2-4754-ab2b-04487467d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import cartopy.crs as ccrs ## map projections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rioxarray\n",
    "from rioxarray.rioxarray import affine_to_coords\n",
    "from pyproj import CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a7c19-075e-432b-9461-36c98031d1c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## from cmethods.utils\n",
    "import warnings\n",
    "from typing import TYPE_CHECKING, Optional, Union, TypeVar\n",
    "\n",
    "XRData_t = (xr.Dataset, xr.DataArray)\n",
    "NPData_t = (list, np.ndarray, np.generic)\n",
    "XRData = TypeVar(\"XRData\", xr.Dataset, xr.DataArray)\n",
    "NPData = TypeVar(\"NPData\", list, np.ndarray, np.generic)\n",
    "MAX_SCALING_FACTOR = 2 ## to allow multiplicative correction?\n",
    "\n",
    "\n",
    "def check_xr_types(obs: XRData, simh: XRData, simp: XRData) -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type xarray.core.dataarray.Dataset or xarray.core.dataarray.DataArray\"\n",
    "\n",
    "    if not isinstance(obs, XRData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, XRData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, XRData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def check_np_types(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Checks if the parameters are in the correct type. **only used internally**\n",
    "    \"\"\"\n",
    "    phrase: str = \"must be type list, np.ndarray or np.generic\"\n",
    "\n",
    "    if not isinstance(obs, NPData_t):\n",
    "        raise TypeError(f\"'obs' {phrase}\")\n",
    "    if not isinstance(simh, NPData_t):\n",
    "        raise TypeError(f\"'simh' {phrase}\")\n",
    "    if not isinstance(simp, NPData_t):\n",
    "        raise TypeError(f\"'simp' {phrase}\")\n",
    "\n",
    "def nan_or_equal(value1: float, value2: float) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the values are equal or at least one is NaN\n",
    "\n",
    "    :param value1: First value to check\n",
    "    :type value1: float\n",
    "    :param value2: Second value to check\n",
    "    :type value2: float\n",
    "    :return: If any value is NaN or values are equal\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    return np.isnan(value1) or np.isnan(value2) or value1 == value2\n",
    "        \n",
    "def ensure_dividable(\n",
    "    numerator: Union[float, np.ndarray],\n",
    "    denominator: Union[float, np.ndarray],\n",
    "    max_scaling_factor: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Ensures that the arrays can be divided. The numerator will be multiplied by\n",
    "    the maximum scaling factor of the CMethods class if division by zero.\n",
    "\n",
    "    :param numerator: Numerator to use\n",
    "    :type numerator: np.ndarray\n",
    "    :param denominator: Denominator that can be zero\n",
    "    :type denominator: np.ndarray\n",
    "    :return: Zero-ensured division\n",
    "    :rtype: np.ndarray | float\n",
    "    \"\"\"\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        result = numerator / denominator\n",
    "\n",
    "    if isinstance(numerator, np.ndarray):\n",
    "        mask_inf = np.isinf(result)\n",
    "        result[mask_inf] = numerator[mask_inf] * max_scaling_factor  # type: ignore[index]\n",
    "\n",
    "        mask_nan = np.isnan(result)\n",
    "        result[mask_nan] = 0  # type: ignore[index]\n",
    "    elif np.isinf(result):\n",
    "        result = numerator * max_scaling_factor\n",
    "    elif np.isnan(result):\n",
    "        result = 0.0\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_pdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Compuites and returns the the probability density function :math:`P(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: The vector to get :math:`P(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`P(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The probability densitiy function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the probability density function :math:`P(x)`\n",
    "\n",
    "        >>> from cmethods get_pdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_pdf(x=x, xbins=xbins))\n",
    "        [2, 5, 5]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    return pdf\n",
    "\n",
    "\n",
    "def get_cdf(\n",
    "    x: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Computes and returns returns the cumulative distribution function :math:`F(x)`\n",
    "    of ``x`` based on ``xbins``.\n",
    "\n",
    "    :param x: Vector to get :math:`F(x)` from\n",
    "    :type x: list | np.ndarray\n",
    "    :param xbins: The boundaries/bins of :math:`F(x)`\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The cumulative distribution function of ``x``\n",
    "    :rtype: np.ndarray\n",
    "\n",
    "\n",
    "    .. code-block:: python\n",
    "        :linenos:\n",
    "        :caption: Compute the cumulative distribution function :math:`F(x)`\n",
    "\n",
    "        >>> from cmethods.utils import get_cdf\n",
    "\n",
    "        >>> x = [1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10]\n",
    "        >>> xbins = [0, 3, 6, 10]\n",
    "        >>> print(get_cdf(x=x, xbins=xbins))\n",
    "        [0.0, 0.16666667, 0.58333333, 1.]\n",
    "    \"\"\"\n",
    "    pdf, _ = np.histogram(x, xbins)\n",
    "    cdf = np.insert(np.cumsum(pdf), 0, 0.0)\n",
    "    return cdf / cdf[-1]\n",
    "\n",
    "\n",
    "def get_inverse_of_cdf(\n",
    "    base_cdf: Union[list, np.ndarray],\n",
    "    insert_cdf: Union[list, np.ndarray],\n",
    "    xbins: Union[list, np.ndarray],\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"\n",
    "    Returns the inverse cumulative distribution function as:\n",
    "    :math:`F^{-1}_{x}\\left[y\\right]` where :math:`x` represents ``base_cdf`` and\n",
    "    ``insert_cdf`` is represented by :math:`y`.\n",
    "\n",
    "    :param base_cdf: The basis\n",
    "    :type base_cdf: list | np.ndarray\n",
    "    :param insert_cdf: The CDF that gets inserted\n",
    "    :type insert_cdf: list | np.ndarray\n",
    "    :param xbins: Probability boundaries\n",
    "    :type xbins: list | np.ndarray\n",
    "    :return: The inverse CDF\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    return np.interp(insert_cdf, base_cdf, xbins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c71f0-69e5-45ad-8663-62c9235338d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def quantile_delta_mapping(\n",
    "    obs: NPData,\n",
    "    simh: NPData,\n",
    "    simp: NPData,\n",
    "    n_quantiles: int,\n",
    "    kind: str = \"+\",\n",
    "    **kwargs,\n",
    "    ) -> NPData:\n",
    "    r\"\"\"\n",
    "    Based on https://python-cmethods.readthedocs.io/en/latest/methods.html#quantile-delta-mapping\n",
    "\n",
    "    kind: str, default + for additive, can be set to * for multiplicative\n",
    "    \"\"\"\n",
    "    # check_adjust_called(\n",
    "    #     function_name=\"quantile_delta_mapping\",\n",
    "    #     adjust_called=kwargs.get(\"adjust_called\"),\n",
    "    # )\n",
    "    check_np_types(obs=obs, simh=simh, simp=simp)\n",
    "\n",
    "    if not isinstance(n_quantiles, int):\n",
    "        raise TypeError(\"'n_quantiles' must be type int\")\n",
    "\n",
    "    if kind=='+':\n",
    "        obs, simh, simp = (\n",
    "            np.array(obs),\n",
    "            np.array(simh),\n",
    "            np.array(simp),\n",
    "        )  # to achieve higher accuracy\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", min(np.nanmin(obs), np.nanmin(simh)))\n",
    "\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = abs(global_max - global_min) / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        # calculate exact CDF values of $F_{sim,p}[T_{sim,p}(t)]$\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "        delta = simp - get_inverse_of_cdf(cdf_simh, epsilon, xbins)  # Eq. 1.3\n",
    "        return QDM1 + delta  # Eq. 1.4\n",
    "\n",
    "    if kind=='*':\n",
    "        obs, simh, simp = np.array(obs), np.array(simh), np.array(simp)\n",
    "        global_max = kwargs.get(\"global_max\", max(np.nanmax(obs), np.nanmax(simh)))\n",
    "        global_min = kwargs.get(\"global_min\", 0.0)\n",
    "        if nan_or_equal(value1=global_max, value2=global_min):\n",
    "            return simp\n",
    "\n",
    "        wide = global_max / n_quantiles\n",
    "        xbins = np.arange(global_min, global_max + wide, wide)\n",
    "\n",
    "        cdf_obs = get_cdf(obs, xbins)\n",
    "        cdf_simh = get_cdf(simh, xbins)\n",
    "        cdf_simp = get_cdf(simp, xbins)\n",
    "\n",
    "        epsilon = np.interp(simp, xbins, cdf_simp)  # Eq. 1.1\n",
    "        QDM1 = get_inverse_of_cdf(cdf_obs, epsilon, xbins)  # Eq. 1.2\n",
    "\n",
    "        delta = ensure_dividable(  # Eq. 2.3\n",
    "            simp,\n",
    "            get_inverse_of_cdf(cdf_simh, epsilon, xbins),\n",
    "            max_scaling_factor=kwargs.get(\n",
    "                \"max_scaling_scaling\",\n",
    "                MAX_SCALING_FACTOR,\n",
    "            ),\n",
    "        )\n",
    "        return QDM1 * delta  # Eq. 2.4\n",
    "    raise NotImplementedError(\n",
    "        f\"{kind=} not available for quantile_delta_mapping. Use '+' or '*' instead.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_cmfunc(\n",
    "    method: str,\n",
    "    obs: XRData,\n",
    "    simh: XRData,\n",
    "    simp: XRData,\n",
    "    **kwargs: dict,\n",
    ") -> XRData:\n",
    "    \"\"\"\n",
    "    Internal function used to apply the bias correction technique to the\n",
    "    passed input data.\n",
    "    \"\"\"\n",
    "    ## hard-code the QDM method\n",
    "    if method!='quantile_delta_mapping':\n",
    "        raise UnknownMethodError('Not implemented for methods other than quantile_delta_mapping')\n",
    "        ## give this a default for what we want to do\n",
    "    else:\n",
    "        method='quantile_delta_mapping' ## not actually going to use this\n",
    "    \n",
    "    check_xr_types(obs=obs, simh=simh, simp=simp)\n",
    "    # if method not in __METHODS_FUNC__:\n",
    "    #     raise UnknownMethodError(method, __METHODS_FUNC__.keys())\n",
    "\n",
    "    if kwargs.get(\"input_core_dims\"):\n",
    "        if not isinstance(kwargs[\"input_core_dims\"], dict):\n",
    "            raise TypeError(\"input_core_dims must be an object of type 'dict'\")\n",
    "        if not len(kwargs[\"input_core_dims\"]) == 3 or any(\n",
    "            not isinstance(value, str) for value in kwargs[\"input_core_dims\"].values()\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                'input_core_dims must have three key-value pairs like: {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}',\n",
    "            )\n",
    "\n",
    "        input_core_dims = kwargs.pop(\"input_core_dims\")\n",
    "    else:\n",
    "        input_core_dims = {\"obs\": \"time\", \"simh\": \"time\", \"simp\": \"time\"}\n",
    "\n",
    "    result: XRData = xr.apply_ufunc(\n",
    "        quantile_delta_mapping,\n",
    "        obs,\n",
    "        simh,\n",
    "        # Need to spoof a fake time axis since 'time' coord on full dataset is\n",
    "        # different than 'time' coord on training dataset.\n",
    "        simp.rename({input_core_dims[\"simp\"]: \"__t_simp__\"}),\n",
    "        dask=\"parallelized\",\n",
    "        vectorize=True,\n",
    "        # This will vectorize over the time dimension, so will submit each grid\n",
    "        # cell independently\n",
    "        input_core_dims=[\n",
    "            [input_core_dims[\"obs\"]],\n",
    "            [input_core_dims[\"simh\"]],\n",
    "            [\"__t_simp__\"],\n",
    "        ],\n",
    "        # Need to denote that the final output dataset will be labeled with the\n",
    "        # spoofed time coordinate\n",
    "        output_core_dims=[[\"__t_simp__\"]],\n",
    "        kwargs=dict(kwargs),\n",
    "    )\n",
    "\n",
    "    # Rename to proper coordinate name.\n",
    "    result = result.rename({\"__t_simp__\": input_core_dims[\"simp\"]})\n",
    "\n",
    "    # ufunc will put the core dimension to the end (time), so want to preserve\n",
    "    # original order where time is commonly first.\n",
    "    return result.transpose(*obs.rename({input_core_dims[\"obs\"]: input_core_dims[\"simp\"]}).dims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fae5d-a8d9-4c62-a6f8-39926ff4bda3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Time utils from Bryan Riel\n",
    "## pasting stuff from iceutils below.\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "def tdec2datestr(tdec_in, returndate=False):\n",
    "    \"\"\"\n",
    "    Convert a decimaly year to an iso date string.\n",
    "    \"\"\"\n",
    "    if isinstance(tdec_in, (list, np.ndarray)):\n",
    "        tdec_list = copy.deepcopy(tdec_in)\n",
    "    else:\n",
    "        tdec_list = [tdec_in]\n",
    "    current_list = []\n",
    "    for tdec in tdec_list:\n",
    "        year = int(tdec)\n",
    "        yearStart = datetime.datetime(year, 1, 1)\n",
    "        if year % 4 == 0:\n",
    "            ndays_in_year = 366.0\n",
    "        else:\n",
    "            ndays_in_year = 365.0\n",
    "        days = (tdec - year) * ndays_in_year\n",
    "        seconds = (days - int(days)) * 86400\n",
    "        tdelta = datetime.timedelta(days=int(days), seconds=int(seconds))\n",
    "        current = yearStart + tdelta\n",
    "        if not returndate:\n",
    "            current = current.isoformat(' ').split()[0]\n",
    "        current_list.append(current)\n",
    "\n",
    "    if len(current_list) == 1:\n",
    "        return current_list[0]\n",
    "    else:\n",
    "        return np.array(current_list)\n",
    "\n",
    "\n",
    "def datestr2tdec(yy=0, mm=0, dd=0, hour=0, minute=0, sec=0, microsec=0, dateobj=None):\n",
    "    \"\"\"\n",
    "    Convert year, month, day, hours, minutes, seconds to decimal year.\n",
    "    \"\"\"\n",
    "    if dateobj is not None:\n",
    "        if type(dateobj) == str:\n",
    "            yy, mm, dd = [int(val) for val in dateobj.split('-')]\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        elif type(dateobj) == datetime.datetime:\n",
    "            attrs = ['year', 'month', 'day', 'hour', 'minute', 'second']\n",
    "            yy, mm, dd, hour, minute, sec = [getattr(dateobj, attr) for attr in attrs]\n",
    "        elif type(dateobj) == np.datetime64:\n",
    "            yy = dateobj.astype('datetime64[Y]').astype(int) + 1970\n",
    "            mm = dateobj.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "            days = (\n",
    "                (dateobj - dateobj.astype('datetime64[M]')) / np.timedelta64(1, 'D')\n",
    "            )\n",
    "            dd = int(days) + 1\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        else:\n",
    "            raise NotImplementedError('dateobj must be str, datetime, or np.datetime64.')\n",
    "\n",
    "    # Make datetime object for start of year\n",
    "    yearStart = datetime.datetime(yy, 1, 1, 0, 0, 0)\n",
    "    # Make datetime object for input time\n",
    "    current = datetime.datetime(yy, mm, dd, hour, minute, sec, microsec)\n",
    "    # Compute number of days elapsed since start of year\n",
    "    tdelta = current - yearStart\n",
    "    # Convert to decimal year and account for leap year\n",
    "    if yy % 4 == 0:\n",
    "        return float(yy) + tdelta.total_seconds() / (366.0 * 86400)\n",
    "    else:\n",
    "        return float(yy) + tdelta.total_seconds() / (365.0 * 86400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d3e1d9-d32c-43d4-98fe-453b57cf72a4",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e2b62c-dc36-4101-b755-09d257c01176",
   "metadata": {},
   "outputs": [],
   "source": [
    "DepthRange         = [0,500]\n",
    "ShallowThreshold   = 100\n",
    "PeriodObs0         = [1950,2015]\n",
    "SelModel = 'CESM'\n",
    "\n",
    "DirSave = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/Summer25Test'\n",
    "DirIn = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles/Summer25Test'\n",
    "\n",
    "DirHadley = f'/Users/eultee/Library/CloudStorage/OneDrive-NASA/Data/gris-iceocean-outfiles'\n",
    "HadleyFile = f'/tf-Hadley-1950_2020.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1016d0-9be8-4745-bfcf-06b0c09b86ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load EN4 using xarray\n",
    "ds1 = xr.open_dataset(DirHadley+HadleyFile, decode_times='timeDim')\n",
    "ds1\n",
    "# ds2 = ds.assign_coords({'timeDim': ds.time, \n",
    "#                   'latDim': ds.lat, \n",
    "#                   'lonDim': ds.lon,\n",
    "#                   'depthDim': ds.depth})\n",
    "\n",
    "# tfEN4 = ds2.tfdpavg0to500_bathymin100.rename({'timeDim': 'time',\n",
    "#                                               'latDim': 'lat',\n",
    "#                                               'lonDim': 'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76909c05-e896-4ead-a285-03f0f42e7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in CESM TF for all time slices available, using multifile dataset\n",
    "with xr.open_mfdataset(f'{DirIn}/tf-CESM2*.nc') as ds: \n",
    "    ds3 = ds.load()\n",
    "\n",
    "ds3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023e615-b000-4f38-bac1-5df8f06f1fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_m = ds3.where(ds3.TF<1e20)\n",
    "ds_m.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574fb8d8-20eb-462e-a0ad-852447dbe9da",
   "metadata": {},
   "source": [
    "Depth resampling seemed to be smearing fill value across depth levels, so that the mean of depth-resampled dataset was ~5e17, even when fill values masked out.  Created `ds_m`  before resampling to address this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696a2339-4bb6-479b-9583-7434c0b11de6",
   "metadata": {},
   "source": [
    "## Express depth in m, then resample to Hadley depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298aff7-1fa0-4bf9-927d-7974b9c709a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## resample CESM to Hadley depths?\n",
    "tf_CESM_inM = ds_m.assign_coords(new_depth = ('lev', ds3.indexes['lev'].values*0.01))\n",
    "tf_CESM_inM = tf_CESM_inM.drop_indexes('lev')\n",
    "tf_CESM_inM = tf_CESM_inM.set_xindex('new_depth').drop_vars('lev')\n",
    "tf_CESM_inM = tf_CESM_inM.rename({'new_depth': 'lev'})\n",
    "tf_CESM_inM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a64ea12-1605-4f9c-bb86-3ff4364b7239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfCESM_resampled = tf_CESM_inM.interp(lev=ds1.depth.values[0:30]).rename({'lev': 'depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8817c1-9f97-436e-a044-28ba5b4af554",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfCESM_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25293c48-28b4-4def-a4ca-85348b366bc6",
   "metadata": {},
   "source": [
    "We had to do one extra step (above) to deal with re-scaling the native depth dimension from cm to m. Then we re-sampled to depth levels that match EN4. Now we proceed with applying a DateTimeIndex, and reprojecting from the rotated pole spatial grid to a regular grid matching EN4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4553ab-0a8e-469a-bca6-7fcec4084128",
   "metadata": {},
   "source": [
    "### Apply DateTimeIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f180b978-d7ed-4119-b9f5-2717422b066d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_full = tfCESM_resampled.TF.sel( \n",
    "                    time=slice('1950', '2020'))\n",
    "\n",
    "## aligning the time indices\n",
    "test_ds_full = test_ds_full.assign_coords(new_time = ('time', test_ds_full.indexes['time'].to_datetimeindex().values))\n",
    "test_ds_full = test_ds_full.drop_indexes('time')\n",
    "test_ds_full = test_ds_full.set_xindex('new_time').drop_vars('time')\n",
    "\n",
    "## aligning the names of the variables between obs and sim\n",
    "test_ds_full = test_ds_full.to_dataset()\n",
    "test_ds_full = test_ds_full.rename({'new_time': 'time'})\n",
    "test_ds_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54f475-c63a-491e-9a70-a886609afd6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tobs_ds_full = ds1.TF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0670518b-f9eb-422e-a48d-9ea25ec49d65",
   "metadata": {},
   "source": [
    "## Reproject obs to match CESM grid -- on depth slices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee14242-e3b1-40ac-80b1-8a2ba4819470",
   "metadata": {},
   "source": [
    "Because the grids are offset from each other, we will need to warp/resample using `rioxarray` before we run the QDM correction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d1244-4a1e-4ac7-8991-5aee6d93b170",
   "metadata": {},
   "source": [
    "Note that the rotated-pole grid of CESM2-WACCM is different from the original test case.  We need to get it on a rectilinear grid before we can use `reproject_match`. Follow [this Gist](https://gist.github.com/j08lue/e792b3c912c33e9191734af7e795b75c) using `rasterio.warp` with `src_geoloc_array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19352004-bb84-4cae-b22f-a45a83335142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6f9f4e-f6f5-4e8a-9695-de3651e7db98",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm that this version has src_geoloc_array option -- must be 1.4 or greater\n",
    "rasterio.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81258e-bd18-436f-ad57-7a81627f8812",
   "metadata": {},
   "source": [
    "#### Set up reusable quantities outside the loop\n",
    "Shape of the original transform, the coordinates to be applied to our new dataset, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac14a33-c9d6-4424-ab04-c06705e92ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lon2d = test_ds_full['lon'].values ## 2d, curvilinear\n",
    "lat2d = test_ds_full['lat'].values\n",
    "\n",
    "lon_EN4 = tobs_ds_full['lon'].values ## 1d, rectilinear grid\n",
    "lat_EN4 = tobs_ds_full['lat'].values\n",
    "\n",
    "WGS84 = rasterio.crs.CRS.from_epsg(4326)\n",
    "\n",
    "## immediately write the spatial reference to tobs_ds_full for later use\n",
    "tobs_ds_full.rio.write_crs(WGS84, inplace=True).rio.set_spatial_dims(\n",
    "    x_dim='lon',\n",
    "    y_dim='lat',\n",
    "    inplace=True,\n",
    ").rio.write_coordinate_system(inplace=True)\n",
    "\n",
    "## set up the dataset transform we will use on every slice\n",
    "dst_transform, dst_width, dst_height = rasterio.warp.calculate_default_transform(\n",
    "    src_crs=WGS84,\n",
    "    dst_crs=WGS84,\n",
    "    width=len(lon_EN4),\n",
    "    height=len(lat_EN4),\n",
    "    src_geoloc_array=(lon2d, lat2d),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ef1dcb-60d1-4553-adff-b93be57b749e",
   "metadata": {},
   "source": [
    "Compute `coords` and `filtered_attrs` to be applied to each slice outside the loop, using a single depth slice.  Used 500 m in prototyping, but this choice shouldn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135932f-ae5d-4a03-a40b-12f7b4b2e91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_ex = test_ds_full['TF'].sel(depth=500, method='nearest').values\n",
    "## go straight to EN4\n",
    "## first value in np.full((t,y,x)) is length of the time dimension\n",
    "destination_ex = np.full((len(test_ds_full.time),\n",
    "                          dst_height, dst_width), np.nan)\n",
    "\n",
    "data_ex, transform_ex = rasterio.warp.reproject(\n",
    "    source_ex,\n",
    "    destination=destination_ex,\n",
    "    src_crs=WGS84,\n",
    "    dst_crs=WGS84,\n",
    "    dst_transform=dst_transform,\n",
    "    dst_nodata=np.nan, ## previously had the Verjans fill value here, but now that we've done\n",
    "    ## a `where` command to mask the dataset before depth resampling, the missing\n",
    "    ## values have been replaced by NaNs\n",
    "    src_geoloc_array=np.stack((lon2d, lat2d))\n",
    ")\n",
    "\n",
    "coords_ex = affine_to_coords(transform_ex, width=dst_width, height=dst_height, x_dim='lon', y_dim='lat')\n",
    "coords_ex.update(time=test_ds_full['time'])\n",
    "\n",
    "filtered_attrs = test_ds_full['TF'].attrs.copy()\n",
    "# filtered_attrs.pop(\"grid_mapping\", None) ## there is no grid_mapping written by default, don't need this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c143a-27d5-4ee8-90d6-d4b4a7e88528",
   "metadata": {},
   "source": [
    "#### Loop over depth levels to make rectilinear and reproject\n",
    "This may take a while, dataset is 14 GB.  In testing it took about 2 minutes on Lizz's NASA laptop.\n",
    "\n",
    "TODO: try implementing with Dask for greater efficiency?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287e805-68fe-4395-bf18-f50220130351",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_slices = {}\n",
    "sim_slices = {}\n",
    "    \n",
    "for d in tobs_ds_full.depth.values[0:30]:\n",
    "    ## now start going rectilinear, then reproject_match\n",
    "    # sim_level = test_ds_full.sel(depth=d, method='nearest')\n",
    "    obs_level = tobs_ds_full.sel(depth=d)\n",
    "\n",
    "    ## get the sim_level on rectilinear grid\n",
    "    source = test_ds_full['TF'].sel(depth=d, method='nearest').values\n",
    "    destination = np.full((len(test_ds_full.time),\n",
    "                              dst_height, dst_width), np.nan)\n",
    "    \n",
    "    data, transform = rasterio.warp.reproject(\n",
    "        source,\n",
    "        destination=destination,\n",
    "        src_crs=WGS84,\n",
    "        dst_crs=WGS84,\n",
    "        dst_transform=dst_transform,\n",
    "        dst_nodata=np.nan, ## previously had the Verjans fill value here, but now that we've done\n",
    "        ## a `where` command to mask the dataset before depth resampling, the missing\n",
    "        ## values have been replaced by NaNs\n",
    "        src_geoloc_array=np.stack((lon2d, lat2d))\n",
    "    )\n",
    "    sim_da = xr.DataArray(data, coords=coords_ex, dims=('time', 'lat', 'lon'), name='TF', attrs=filtered_attrs)\n",
    "    sim_level = sim_da.where(sim_da<1.0e20) ## remove fill values\n",
    "    sim_level.rio.write_crs(WGS84, inplace=True).rio.set_spatial_dims(\n",
    "        x_dim='lon',\n",
    "        y_dim='lat',\n",
    "        inplace=True,\n",
    "    ).rio.write_coordinate_system(inplace=True)\n",
    "    \n",
    "    obs_repr = obs_level.rio.reproject_match(sim_level)\n",
    "    obs_slices[d] = obs_repr\n",
    "    # obs_slices[d] = {'dims': (\"time\", \"lat\", \"lon\"), 'data': obs_repr.data}\n",
    "    sim_slices[d] = sim_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8edd442-0df8-400b-a6af-8f479667cafe",
   "metadata": {},
   "source": [
    "#### Concatenate to make nice dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa22aa-4b98-4395-bd7f-bddde0d05434",
   "metadata": {},
   "outputs": [],
   "source": [
    "obtemp = xr.concat([obs_slices[d] for d in tobs_ds_full.depth.values[0:30]], dim=tobs_ds_full.depth.values[0:30])\n",
    "obtemp = obtemp.drop_vars('depth') ## drop a 1D depth variable that carried through\n",
    "simtemp = xr.concat([sim_slices[d] for d in tobs_ds_full.depth.values[0:30]], dim=tobs_ds_full.depth.values[0:30])\n",
    "# simtemp = simtemp.drop_vars('depth') ## problem var is not present in sim\n",
    "\n",
    "tobs_repr_match = obtemp.rename({'concat_dim': 'depth', 'x': 'lon', 'y': 'lat'})\n",
    "tsim_match = simtemp.rename({'concat_dim': 'depth'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d166bf7-ce53-4cc2-b5ff-0f92a16a6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_repr_match = tobs_repr_match.to_dataset(name='TF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de21c2-cdfe-47dd-8f26-e7b1d07cbe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm this is a Dataset\n",
    "tsim_match = tsim_match.to_dataset(name='TF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3620b8c-b7cc-4e94-997a-af670d7b93bc",
   "metadata": {},
   "source": [
    "This worked!  Try the QDM to make sure it will work as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d11b7f-69a4-4aac-9bdf-fc1d49161f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
