{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c1a6e2e-7a18-47de-a1ae-1c74e8b04668",
   "metadata": {},
   "source": [
    "# Toy QDM\n",
    "This notebook is to debug QDM implementation. Use cmethods or modify its source code directly.\n",
    "\n",
    "16 Jan 2024 | EHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c64141-5fc4-42ca-8e29-852a5040b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sys.path.append('/home/theghub/ehultee/ISMIP7-utils/python-cmethods')\n",
    "from cmethods import adjust\n",
    "\n",
    "# from verjansFunctions import qmProjCannon2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348fe1a-a44a-49b8-9c31-ab08aea8f7a2",
   "metadata": {},
   "source": [
    "Initial run settings from Vincent.  Replace most of this with our own file selection, eventually.  Just check that this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ece6e-973b-4394-a22d-75b47ea5627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DepthRange         = [0,500]\n",
    "ShallowThreshold   = 100\n",
    "PeriodObs0         = [1950,2015]\n",
    "SigmaExclusion     = 4 #number of sdevs beyond which we constrain values in QDM\n",
    "yrWindowProj       = 30 #number of years running window CDF in projection period \n",
    "\n",
    "# DirEN4         = f'{cwd}Verjans_InputOutput/'\n",
    "# EN4file        = f'dpavg_tf_EN4anl_Dp{DepthRange[0]}to{DepthRange[1]}_bathymin{ShallowThreshold}.nc'\n",
    "DirEN4         = f'/Users/eultee/Downloads/'\n",
    "EN4file        = f'dpavg_tf_EN4anl_Dp{DepthRange[0]}to{DepthRange[1]}_bathymin{ShallowThreshold}.nc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a226a-64c8-4d6f-9d13-5714896c24ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load EN4 using xarray\n",
    "ds = xr.open_dataset(DirEN4+EN4file, decode_times='timeDim')\n",
    "ds2 = ds.assign_coords({'timeDim': ds.time, \n",
    "                  'latDim': ds.lat, \n",
    "                  'lonDim': ds.lon,\n",
    "                  'depthDim': ds.depth})\n",
    "ds2\n",
    "\n",
    "tfEN4 = ds2.tfdpavg0to500_bathymin100.rename({'timeDim': 'time',\n",
    "                                              'latDim': 'lat',\n",
    "                                              'lonDim': 'lon'})\n",
    "tfEN4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a0f6d-eb93-4637-a593-5beb921f2f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "## Bryan Riel, please save me. Decimal year to datetime is the bane of this notebook.\n",
    "## pasting stuff from iceutils below.\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "def tdec2datestr(tdec_in, returndate=False):\n",
    "    \"\"\"\n",
    "    Convert a decimaly year to an iso date string.\n",
    "    \"\"\"\n",
    "    if isinstance(tdec_in, (list, np.ndarray)):\n",
    "        tdec_list = copy.deepcopy(tdec_in)\n",
    "    else:\n",
    "        tdec_list = [tdec_in]\n",
    "    current_list = []\n",
    "    for tdec in tdec_list:\n",
    "        year = int(tdec)\n",
    "        yearStart = datetime.datetime(year, 1, 1)\n",
    "        if year % 4 == 0:\n",
    "            ndays_in_year = 366.0\n",
    "        else:\n",
    "            ndays_in_year = 365.0\n",
    "        days = (tdec - year) * ndays_in_year\n",
    "        seconds = (days - int(days)) * 86400\n",
    "        tdelta = datetime.timedelta(days=int(days), seconds=int(seconds))\n",
    "        current = yearStart + tdelta\n",
    "        if not returndate:\n",
    "            current = current.isoformat(' ').split()[0]\n",
    "        current_list.append(current)\n",
    "\n",
    "    if len(current_list) == 1:\n",
    "        return current_list[0]\n",
    "    else:\n",
    "        return np.array(current_list)\n",
    "\n",
    "time_arr = tdec2datestr(tfEN4.time.values)\n",
    "time_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947cffb-5c59-4db8-a940-08b05756f311",
   "metadata": {},
   "source": [
    "Okay, finally successfully converted.  We need the obs dataset to have the same time type as the modeled one in order to use QDM `adjust`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ada0eb-58da-4b5a-a69f-44be46d805f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now we try the cmethods Quantile Delta Mapping.  See [example notebook](https://github.com/ehultee/gris-iceocean-process/blob/main/python-cmethods_examples.ipynb) added by DF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174eaf4-2fbd-431d-8f0a-da54415b0b50",
   "metadata": {},
   "source": [
    "QDM `adjust` from cmethods needs datasets defined and input as simulated historical (`simh`), simulated projection (`simp`), and observed historical against which to bias-correct (`obs`).\n",
    "\n",
    "Slice the EN4 dataset for the obs period defined by Vincent's `PeriodObs0`.  Import the example dataset of CESM2 TF for the same depth range and bathymetric threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea3237-a5b8-4bb3-9d12-f3e17f10081d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds3 = xr.open_dataset(DirEN4+'/tfdpavg-CESM2-2024-11-14.nc')\n",
    "ds3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c579be0-f713-4385-bf61-9e9ba809eb5d",
   "metadata": {},
   "source": [
    "This is a short example dataset.  For the sake of argument, let's take a very short correction period over the first half, and use the second half as the projection.  Let's try at a single grid cell to get our bearings.\n",
    "\n",
    "---\n",
    "### Try QDM on a 1D series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d080a-83b5-42b8-842f-888e321fae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## select a single site\n",
    "lat_sel = 65.5 ## deg N\n",
    "lon_sel = 0.5 ## deg E\n",
    "\n",
    "test_series = ds3.TF.sel(lon=lon_sel, lat=lat_sel, method='nearest')\n",
    "\n",
    "test_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4759c8b-63dc-4b6d-98b7-096afde5ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_obs = tfEN4.sel(lat=lat_sel, lon=lon_sel, method='nearest')\n",
    "test_obs_trimmed = test_obs.sel(time=slice('2000', '2014'))\n",
    "test_obs_trimmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a6be22-412c-4645-ab39-70c4267655e6",
   "metadata": {},
   "source": [
    "In order to plot these series together, we need both to have a date type matplotlib recognizes. You would think it would be enough to convert one of them (EN4) but now the other is not behaving, so here we are.  Importing the inverse function from [Bryan Riel](https://github.com/bryanvriel/iceutils/blob/master/iceutils/timeutils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b3bcf3-bdd1-48ee-9806-7b36bd3986ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def datestr2tdec(yy=0, mm=0, dd=0, hour=0, minute=0, sec=0, microsec=0, dateobj=None):\n",
    "    \"\"\"\n",
    "    Convert year, month, day, hours, minutes, seconds to decimal year.\n",
    "    \"\"\"\n",
    "    if dateobj is not None:\n",
    "        if type(dateobj) == str:\n",
    "            yy, mm, dd = [int(val) for val in dateobj.split('-')]\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        elif type(dateobj) == datetime.datetime:\n",
    "            attrs = ['year', 'month', 'day', 'hour', 'minute', 'second']\n",
    "            yy, mm, dd, hour, minute, sec = [getattr(dateobj, attr) for attr in attrs]\n",
    "        elif type(dateobj) == np.datetime64:\n",
    "            yy = dateobj.astype('datetime64[Y]').astype(int) + 1970\n",
    "            mm = dateobj.astype('datetime64[M]').astype(int) % 12 + 1\n",
    "            days = (\n",
    "                (dateobj - dateobj.astype('datetime64[M]')) / np.timedelta64(1, 'D')\n",
    "            )\n",
    "            dd = int(days) + 1\n",
    "            hour, minute, sec = [0, 0, 0]\n",
    "        else:\n",
    "            raise NotImplementedError('dateobj must be str, datetime, or np.datetime64.')\n",
    "\n",
    "    # Make datetime object for start of year\n",
    "    yearStart = datetime.datetime(yy, 1, 1, 0, 0, 0)\n",
    "    # Make datetime object for input time\n",
    "    current = datetime.datetime(yy, mm, dd, hour, minute, sec, microsec)\n",
    "    # Compute number of days elapsed since start of year\n",
    "    tdelta = current - yearStart\n",
    "    # Convert to decimal year and account for leap year\n",
    "    if yy % 4 == 0:\n",
    "        return float(yy) + tdelta.total_seconds() / (366.0 * 86400)\n",
    "    else:\n",
    "        return float(yy) + tdelta.total_seconds() / (365.0 * 86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3ce23-5299-42a3-af71-e31f355f0168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## should we try expressing both with to_datetimeindex...?\n",
    "# test_obs_trimmed.indexes['time'].to_datetimeindex()\n",
    "tobs_times = pd.to_datetime(tdec2datestr(test_obs_trimmed.time.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2d0a1-f00e-48b0-a2df-09f5bea84463",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# ax.plot(test_series.time.values, test_series)\n",
    "ax.plot(test_series.indexes['time'].to_datetimeindex().values, test_series, label='CESM2')\n",
    "ax.plot(tobs_times,\n",
    "    test_obs_trimmed, label='EN4')\n",
    "ax.legend(loc='best')\n",
    "ax.set(xlabel='Year', ylabel='Thermal forcing', title='Series extracted for example cell ({} E,{} N)'.format(lat_sel, lon_sel))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0e7c20-116a-48e4-a245-079b806167aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_series = test_obs_trimmed.assign_coords(new_time = ('time', tobs_times))\n",
    "obs_series = obs_series.drop_indexes('time')\n",
    "obs_series_1 = obs_series.set_xindex('new_time').drop_vars('time')\n",
    "obs_series_1 = obs_series_1.rename({'new_time': 'time'})\n",
    "obs_series_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b61606f-d395-4beb-b7af-e63539f54d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_series = test_series.assign_coords(new_time = ('time', test_series.indexes['time'].to_datetimeindex().values))\n",
    "sim_series = sim_series.drop_indexes('time')\n",
    "sim_series_1 = sim_series.set_xindex('new_time').drop_vars('time')\n",
    "sim_series_1 = sim_series_1.rename({'new_time': 'time'})\n",
    "sim_series_1 \n",
    "## = sim_series_1.sel(time=slice('2000','2013')) ## maybe the underlying data has to be exactly the same length? alignment otherwise seems good..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32d117-6b61-4170-ada9-a09095b9c7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_ds = obs_series_1.to_dataset()\n",
    "sim_ds = sim_series_1.to_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee919fdf-8cdf-4071-886b-235f5ec90308",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ds = sim_ds.rename({'TF': 'tfdpavg0to500_bathymin100'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5e5734-16cc-407a-b51b-2ba37190dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's try QDM adjustment here\n",
    "# to adjust a 3d dataset\n",
    "qdm_result = adjust(\n",
    "    method = \"quantile_delta_mapping\",\n",
    "    obs = obs_ds.sel(time=slice('2000','2007')),\n",
    "    simh = sim_ds.sel(time=slice('2000', '2007')).rename({'time':'t_simh'}),\n",
    "    simp = sim_ds.sel(time=slice('2007', '2014')),\n",
    "    n_quantiles = 100,\n",
    "    input_core_dims={\"obs\": \"time\", \"simh\": \"t_simh\", \"simp\": \"time\"},\n",
    "    # group={\"obs\": \"time.month\", \"simh\": \"t_simh.month\", \"simp\": \"time\"},\n",
    "    kind = \"+\", # to calculate the relative rather than the absolute change, \"*\" can be used instead of \"+\" (this is prefered when adjusting precipitation)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bf0c28-9d75-4a87-b716-a2bf047bb929",
   "metadata": {},
   "source": [
    "---\n",
    "## Test QDM on a 3D dataset instead of a series\n",
    "We are getting \n",
    "```\n",
    "AttributeError: 'Dataset' object has no attribute 'to_dataset'\n",
    "```\n",
    "\n",
    "Perhaps this is because we've fed the function a series converted to a Dataset, rather than a 3D dataset?  Take a small slice to try."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e63d1c8-39ed-4780-aebb-a7729acfde3b",
   "metadata": {},
   "source": [
    "### Process a small subset of simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36e9fa-0183-44ec-b026-fbc66e9987ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try to do all the steps in pre-processing at once...\n",
    "test_ds = ds3.TF.sel(lon=slice(lon_sel+0.1, lon_sel+1), lat=slice(lat_sel-1, lat_sel+2))\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bdbe79-6b3d-4b7a-8aaa-aef34743ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aligning the time indices\n",
    "test_ds = test_ds.assign_coords(new_time = ('time', test_ds.indexes['time'].to_datetimeindex().values))\n",
    "test_ds = test_ds.drop_indexes('time')\n",
    "test_ds = test_ds.set_xindex('new_time').drop_vars('time')\n",
    "\n",
    "## aligning the names of the variables between obs and sim\n",
    "test_ds = test_ds.to_dataset()\n",
    "test_ds = test_ds.rename({'new_time': 'time', 'TF': 'tfdpavg0to500_bathymin100'})\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678dd91-502f-46a7-bd8c-98ac574fbd9b",
   "metadata": {},
   "source": [
    "### Process small subset of reanalysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8944a8-cf43-4da1-ac21-b7e9de0c554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_ds = tfEN4.sel(lon=slice(lon_sel-1, lon_sel+1), \n",
    "                    lat=slice(lat_sel-2, lat_sel+2), \n",
    "                    time=slice('2000', '2014'))\n",
    "tobs_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0b68c-2375-4a30-af0e-da68307bdbd2",
   "metadata": {},
   "source": [
    "We are back to the problem of offset grids.  Ideally we would do a point-by-point implementation that uses nearest neighbors, because xarray is good at this.  Realigning the whole grid is less efficient.  For now, let's see if it will work to just force the same grid -- this tells us whether the problem is even worth solving for cmethods implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3882d3f1-1d5f-4fa5-8e6c-08be4ab51627",
   "metadata": {},
   "outputs": [],
   "source": [
    "scam_lat = [v+0.5 for v in tobs_ds.lat.values]\n",
    "scam_lon = [v+0.5 for v in tobs_ds.lon.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7210258-4c81-4bba-a8e8-5339a2e16d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "## overwrite them\n",
    "tobs_ds = tobs_ds.assign_coords(new_lat = ('lat', scam_lat))\n",
    "tobs_ds = tobs_ds.drop_indexes('lat')\n",
    "tobs_ds = tobs_ds.set_xindex('new_lat').drop_vars('lat')\n",
    "\n",
    "tobs_ds = tobs_ds.assign_coords(new_lon = ('lon', scam_lon))\n",
    "tobs_ds = tobs_ds.drop_indexes('lon')\n",
    "tobs_ds = tobs_ds.set_xindex('new_lon').drop_vars('lon')\n",
    "\n",
    "tobs_ds = tobs_ds.rename({'new_lat': 'lat', 'new_lon': 'lon'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291960d6-1aee-4be4-b774-3668f7831f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa3799a-0a57-4413-80bb-2825b378ca70",
   "metadata": {},
   "source": [
    "Reset the time index to be a datetime type.  Note that these data are also float32 rather than float64.  Could cause problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa1796-73a0-4ef8-8aee-60ddadd80527",
   "metadata": {},
   "outputs": [],
   "source": [
    "tobs_ds = tobs_ds.assign_coords(new_time = ('time', pd.to_datetime(tdec2datestr(tobs_ds.time.values))))\n",
    "tobs_ds = tobs_ds.drop_indexes('time')\n",
    "tobs_ds = tobs_ds.set_xindex('new_time').drop_vars('time')\n",
    "tobs_ds = tobs_ds.rename({'new_time': 'time'})\n",
    "tobs_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4029c4a-20e7-442f-a2b9-9d768fa41f6c",
   "metadata": {},
   "source": [
    "### Attempt QDM on these 3D sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415b4531-09d1-4d20-86a7-888fb1b00eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdm_result = adjust(\n",
    "    method = \"quantile_delta_mapping\",\n",
    "    obs = tobs_ds.sel(time=slice('2000','2007')),\n",
    "    simh = test_ds.sel(time=slice('2000', '2007')).rename({'time':'t_simh'}),\n",
    "    simp = test_ds.sel(time=slice('2007', '2014')),\n",
    "    n_quantiles = 100,\n",
    "    input_core_dims={\"obs\": \"time\", \"simh\": \"t_simh\", \"simp\": \"time\"},\n",
    "    # group={\"obs\": \"time.month\", \"simh\": \"t_simh.month\", \"simp\": \"time\"},\n",
    "    kind = \"+\", # to calculate the relative rather than the absolute change, \"*\" can be used instead of \"+\" (this is prefered when adjusting precipitation)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4613d5-4833-4259-b040-d2c96a74f5fc",
   "metadata": {},
   "source": [
    "The source code warns that this is disabled (??), and the line is only called if the group argument is not set.  So, fun-lovers that we are, we try to use a grouping suggested in the cmethods docs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0112c0-7f0d-47b8-bb31-8f8a6c7557c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "qdm_result = adjust(\n",
    "    method = \"quantile_delta_mapping\",\n",
    "    obs = tobs_ds.sel(time=slice('2000','2007')),\n",
    "    simh = test_ds.sel(time=slice('2000', '2007')).rename({'time':'t_simh'}),\n",
    "    simp = test_ds.sel(time=slice('2007', '2014')),\n",
    "    n_quantiles = 100,\n",
    "    input_core_dims={\"obs\": \"time\", \"simh\": \"t_simh\", \"simp\": \"time\"},\n",
    "    group={\"obs\": \"time.month\", \"simh\": \"t_simh.month\", \"simp\": \"time\"},\n",
    "    kind = \"+\", # to calculate the relative rather than the absolute change, \"*\" can be used instead of \"+\" (this is prefered when adjusting precipitation)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc073754-e697-4fcd-92ae-8e795b47c064",
   "metadata": {},
   "source": [
    "...but we can't use group for distribution-based methods.  Alas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7366bb9f-b25a-4ddc-856c-701bd477df9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
